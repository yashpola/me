{
  "Years": [
    {
      "Y3S1": [
        {
          "code": "CS2109S",
          "title": "Introduction to AI/ML",
          "generalreview": "http://disq.us/p/3188fkj",
          "thumbnail": "https://www.cio.com/wp-content/uploads/2025/02/3829539-0-75501800-1740132217-shutterstock_2482705481.jpg?quality=50&strip=all",
          "meme": "https://pbs.twimg.com/media/FSDfnEyUcAEUMSd.jpg:large",
          "prologue": [
            "Going in to this course, I had honestly no better clue of how AI/ML ",
            "works than than layman despite the great amount of recent public exposure. Leaving however, I ",
            "had at least an understanding of the basic motivations of how AI/ML is ",
            "exercised. Let me start with a brief recap of the whole course."
          ],
          "para1": [
            "As a simpler exercise: how might an artificial agent answer a simple, factual question? Fortunately, this is easy: ",
            "the agent could reduce the question to a search problem and \"find\" the answer in a deterministic manner. Take this query: ",
            "provide the smallest number of movies that connect Timothée Chalamet and Robert Pattinson (the answer is 2: Timothée acted with Zendaya in Dune, ",
            "and Zendaya with Rob in The Drama). To program an agent to compute this, we clearly first need data supporting the query (in this case, actors and movies). ",
            "Then, we may represent the data in terms of nodes & arcs (example figure below) and simply run the Breadth-First Search algorithm to retrieve our desired path ",
            "(namely, Tim -> Zendaya -> Rob). While the example I used was simple, in essence the task is to create states representing data, define ",
            "transitions between them (which may or may not have variable costs, depending on our problem definition), and define start and end points."
          ],
          "para2": [
            "Now, a more complicated ask: how might an artificial agent make decisions/predictions? This is not so easy, but it comes down to math. ",
            "More generally - we have feature observations (x) which, through some unknown polynomial combination involving unknown weights (w), map ",
            "to outputs (y) which we try to approximate through a hypothesis function that produces predictions (y'). The task is really to find the ",
            "best values for the unknown w, which of-course requires first devising a good hypothesis function (prediction line/polynomial combination). ",
            "What could x and y represent though? Well, likely anything that can be represented as a finite set of attributes over a numerical interval. ",
            "Two examples: (1) predicting house prices (y) based on location, size, and age (x) and (2) predicting whether a tumor is benign or malignant (y) ",
            "based on diameter in cm (x). In this course, I learnt of two broad techniques: classification, and regression. Classification, as the name ",
            "suggests, is the task of labelling/discretizing data while regression (as I learnt it) refers to (usually continuous) aggregation ",
            "over numerical data. For regression tools, I learned of Decision Trees & Linear Regression. For classification, I learned of Logistic ",
            "Regression, Support Vector Machines, Perceptrons, and Neural Networks. While both techniques have their use cases, they may sometimes ",
            "be used in place of other (ML is a very flexible field!). The main difference between these techniques is in their objective functions (used ",
            "to find the best weights (w), and the complexities with which they describe the relationship between x, w, and y (prediction lines). ",
            ""
          ],
          "para3": [
            "At the risk of elaborating too much however, I will comment briefly - before closing - on just one of the topics I found to be the most cool: ",
            "Support Vector Machines (SVMs). First, consider the motivation: we want as confident a classification as possible of training samples x i.e. ",
            "we do not want our model to predict 1 for 0, and 0 for 1 (as an example). Thus, we focus on what is called the \"margin\" (read: distance) ",
            "between the two (I say two because in this course we mostly deal with binary classification) classes of observations and aim to maximise it. ",
            "Of course, this goal is formalised through an objective function (specifically, the Lagrangian quadratic program). I find this cool because (1) I feel ",
            "it really exemplifies the elegance of ML as making creative use of (conceptually) simple algebra to do most of the work, and (2) combines several ",
            "enthralling concepts together (to say the least: calculus, quadratic programming, primal/dual, and algebra). To end off with a quote by the late ",
            "Patrick Winston from MIT: \"You would think that after people have been working on this sort of stuff for 50 or 75 years, there wouldn’t be any ",
            "tricks in the bag left. But that’s when everybody gets surprised.\""
          ]
        },
        {
          "code": "CS3231",
          "title": "Theory of Computation",
          "generalreview": "http://disq.us/p/3188oot",
          "thumbnail": "https://miro.medium.com/v2/resize:fit:880/0*hupeBBgcYLauBXNr.png",
          "meme": "https://preview.redd.it/51c2i8p9ovy31.png?auto=webp&s=5f1ccf07f0d843b344748912f09d16eb86349691",
          "prologue": [
            "A course unlike any CS course I've done or will ever do. Theory of Computation was all about generalising (rigorously) the computational ",
            "power of the machine. Think: what exactly does it mean for a machine to \"compute\" the answer to a problem? Sure, your Algorithms class ",
            "enumerated and proved to you how certain routines solve certain problems. But how exactly do routines translate in a machine? Probably, your ",
            "Computer Architecture class enlightened you to the data and control paths. Then, your OS class swooped in to give you process abstraction. ",
            "These are obviously crucial, but what lies even further beyond? What is the most general representation of a machine? ",
            "Or indeed, even of a problem that has a specific solution? What exactly can('t) a machine compute? These questions, as (basically) a historical ",
            "review of computational innovation, are devised and answered by the theory of computing. In this recap, I aim - rather ambitiously - to recap the ",
            "the course content in 3 paragraphs, though I acknowledge now that there will be several many concepts I leave out or explain crudely. But perhaps ",
            "the task is just Hard ;)"
          ],
          "para1": [
            "First thing to note: we deal - more generally - in \"languages\" instead of \"problems\", and talk about \"language acceptance\" ",
            "instead of \"problem solving\" (in the traditional/algorithms sense). Simply put: a machine is given a \"string\" as an input ",
            "and is asked to verify the string's membership in a specific language. If the machine \"accepts\" only those strings that belong to ",
            "that language (nothing more or less), then it is said to accept the language as a whole. Further, the language \"of\" the machine is ",
            "said to be that language. Now, for some context: a string is simply an ordered sequence of symbols (such as abab) over a fixed, finite ",
            "alphabet (a predefined set of symbols such as {a, b}), and a language is some countable set of strings over an alphabet (such as ",
            "{abab, abbb, aa}). Therefore, when I talk about the \"expressive power\" of a machine, I'm really just referring to the languages ",
            "that such a machine is able to accept. As such, a large part of this course was really just defining and devising machines with increasing ",
            "levels of expressive power. We start with the basic: deterministic and nondeterministic finite automata (DFAs/NFAs). While these are a good ",
            "introduction to HOW exactly we define language acceptance for a machine (i.e. in terms of states & transitions). These, as the name ",
            "suggests, are both finite in memory (required to store symbols), and thus the same in expressive power (although NFAs are more convenient ",
            "to work with). To then give ourselves memory, we introduce Push-Down Automata (PDA). These are essentially the same as DFAs/NFAs, except ",
            "they now have exactly 1 infinite stack to store string symbols in the order they are received, and retrieve them in the reverse order. Are ",
            "there limitations for PDAs too though? Of course! But let's see how we can know this."
          ],
          "para2": [
            "We must prove that each type of machine really can't do what we claim it can't (for instance, no finite automaton can accept a language ",
            "where all strings contain an equal number, n, of a's and b's for n >= 0). Fortunately, as with most things, we stand on the shoulders of ",
            "giants. We use a very powerful tool, called the Pumping Lemma, that helps us prove either that a language is NOT regular (i.e. cannot be ",
            "accepted by a DFA/NFA) or NOT context-free (i.e. cannot be accepted by a PDA) depending on which variant we use. This is because all regular ",
            "/context-free languages satisfy the Pumping Lemma for regular/context-free languages. Thereby, the PL becomes a necessary (but not ",
            "sufficient) condition for regular/context-free languages. Finally, before commenting on Turing Machines, I note also two additional constructs ",
            "taught in this course: regular expressions, and grammars. In fact, to put it simply: regex, grammars, and automata are all varying ways ",
            "of expressing languages, and are closely related to each other (for example, (1) all regular expressions correspond exactly to the set of ",
            "of regular languages and can be expressed as a finite automaton or (2) context-free grammars - such as the one below - correspond exactly ",
            "to the set of context-free languages and can be expressed as a push-down automaton)."
          ],
          "para3": [
            "Turing Machines offer the highest level of expressive power. These gizmos consist minimally of tapes (with blank cells to write symbols into) ",
            "and a central finite control to do the actual reading, writing, and string acceptance/rejection (called halting for TMs). TMs offer the same ",
            "expressive power over all variants (single-tape; picture below, multi-tape, multi-track, etc), with these only bringing convenience-benefits ",
            "like NFAs do over DFAs. Once the course introduced the Turing Machine, it could then talk about much more sophisticated issues than just language ",
            "acceptance & rejection: namely, the decideability, and complexity of problems. I'll present just one undecideable problem, for the sake of brevity :), which is the ",
            "Halting Problem. This language involves the set of all arbitrary strings (w) and some Turing Machine (M1) and asks if M1 halts on w (i.e. accepts/",
            "rejects it in time finite. Is it obvious that this problem is undecidable (i.e. no TM halts on every string in the language)? If its not, then consider this informal ",
            "proof: suppose (*) there's some machine M2 that solves this problem, and run M2 in this way - [if M1 halts on input w, then keep M2 running ",
            "forever]. This exposes a contradiction since M2 won't halt whether or not M1 halts, but by supposition (*), M2 should've halted!. ",
            "There are many such undecideable problems of which some covered in this course are: The Diagonalisation Language, the Universal Language, ",
            "The Empty Language, etc. In fact, we were taught formal properties of a language as it relates to hardness, two being recursive enumerability ",
            "(i.e. a TM halts on all valid inputs for a language), and recursiveness (i.e. a TM halts on all inputs), which makes the undecideable languages effectively ",
            "at least non-recursive. Similarly, complexity - specifically, time/space costs - are defined similarly in a TM-computation sense, and ",
            "concomitantly so is the concept of NP/NP-Hard/NP-Complete. As a last remark, there is the actual Universal Turing Machine proposed originally ",
            "by Alan Turing - which claims to be able to decide any recursive language & compute any recursively enumerable language and that might be one of the ",
            "cooler parts of a load of content which is already groundbreaking to me."
          ]
        },
        {
          "code": "CS4226",
          "title": "Internet Architecture",
          "generalreview": "http://disq.us/p/3188z62",
          "thumbnail": "https://www.conceptdraw.com/How-To-Guide/picture/Network-diagram-Communication-network-architecture.png",
          "meme": "https://i.imgflip.com/1rivut.jpg",
          "prologue": [""],
          "para1": [""],
          "para2": [""],
          "para3": [""]
        },
        {
          "code": "CS4234",
          "title": "Optimisation Algorithms",
          "generalreview": "http://disq.us/p/31898gp",
          "thumbnail": "https://images.prismic.io/sketchplanations/611d51e9-25b7-4d00-a800-ccde2e672a39_190464809359.jpg?auto=compress,format",
          "meme": "https://i.redd.it/vehc07hlgmr81.jpg",
          "prologue": [
            "The basic Algorithm Design course in NUS stops at defining NP-Hardness (recall: a problem which can be reduced in polynomial time from ",
            "any problem that is solvable in non-deterministic polynomial time i.e. the NP class), and introducing the P=NP problem (can any problem ",
            "in NP be solved in deterministic polynomial time?). Optimisation Algorithms then takes the baton from here, and proposes three relaxations ",
            "for optimising (minimising/maximising) over a specific subset of NP-Hard problems called combinatorial problems. These relaxations are over ",
            "(1) runtime (accept a reasonably nonpolynomial solution), (2) correctness (accept a reasonable approximation of the optimal solution), ",
            "and (3) generality (solve only special cases of an NP-Hard problem in deterministic polynomial time)."
          ],
          "para1": [
            "The concept of approximating the solution to a combinatorial optimisation problem was introduced using Minimum Vertex Cover (recall: ",
            "given vertices V and edges E, find a subset of V of minimum cardinality such that it includes at least 1 endpoint of every edge E). MVC ",
            "is known to be NP-Hard but, as stated above, an optimal solution can be approximated through several methods (a greedy 2-approximation ",
            "is a notable one). A 2-approximation of VC would therefore mean that the computed vertex cover has at most twice as many vertices ",
            "as the optimal one. Equally, if we wanted to maximise over something, then a 2-approximation would intuitively mean at least half of the ",
            "the optimal solution is achieved. Note that an approximation can be within a constant (such as 2 or 3) or function (such as logn). Some ",
            "problems are in-fact hard even to polynomially approximate, such as the Euclidean+No-Repeats Travelling Salesman Problem. Luckily these, ",
            "are few and far between in terms of practical use in the real world. The course also covered approximations for a host of problems including ",
            "but not limited to - Minimum Set Cover, the Travelling Salesman Problem, CNF-SAT, and Maximum Cut. In the end, my favorite approximation ",
            "algorithm was the Christofides-Serdyukov 1.5-approximation algorithm for TSP. I surmise I will forever be in awe of how this 12-step algorithm ",
            "that itself made use of minimum spanning trees, euclidean cycles, and edmonds' matching was even conceived of! Perhaps \"favorite\" is a ",
            "bit of a misnomer though, since I absolutely also loved Linear Programming which is a very simple technique that involves representing ",
            "the constraints and features of an combinatorial optimisation problem as a linear system, and solving it. Of course, since these problems ",
            "remain NP-Hard even for the linear system, the Linear Program often also has to be relaxed to retain the polynomial time bound."
          ],
          "para2": [
            "Separately, solving only a special case of an NP-Hard problem in polynomial time was illustrated using Maximum Matching (which, by the way, is ",
            "a mirror problem to Minimum Vertex Cover) but only in Bipartite Graphs. To work our way up to this though, we started with a very famous ",
            "theorem in optimisation: Maximum Flow-Minimum Cut (MFMC). This theorem states that in a network comprised of links each with some nonzero capacity, ",
            "a source s and a sink t, the maximum flow (i.e. the max outflow from s/inflow to t) is equal to the minimum cut (i.e. the set of edges with ",
            "minimum sum capacity that need to be removed to disconnect all paths from s to t). We first re-constructed the hallmark Ford-Fulkerson ",
            "method for finding the maximum flow/minimum cut of a network as well as practical implementations of this method (Edmonds-Karp Fat Pipes ",
            "& Dinitz's) which run in time polynomial (O(V^2E) to be explicit). Then, we applied MFMC (almost verbatim!) to special problems such as ",
            "maximum matching in bipartite graphs, minimum vertex-disjoint path cover, etc. It's great that we can do this since maximum bipartite matching ",
            "is itself a very powerful technique to easily solve combinatorial problems (for example: if I have X customers with Y sets of needs and Z ",
            "agents, how do I select the minimum number of agents such that all customers' needs are met?)."
          ],
          "para3": [
            "Notice that runtime, correctness, and generality are usually traded-off for each other i.e. BECAUSE we are okay with only an approximation ",
            "of an optimal solution, we can sketch out a solution that runs faster (in this sense, \"solutions\" begin to resemble \"heuristics\"). Finally, ",
            "as an aside - the course also covered some miscalleneous content on probability-based methods. There was a quick recap of naive randomization",
            "(often involved crudely assigning probabilities to events based on an intended outcome). But of course, we went beyond to study more refined ",
            "tricks such as the Lovász Local Lemma (which places a bound on the probability space by judging the indepence of certain events, and ",
            "derandomization using the method of conditional probabilities (very cool! almost a hack). I am, however, looking forward to NUS' course on ",
            "Randomized Algorithms (CS5330) to find out more beyond what little Optimisation has covered. All in all, a very encyclopedic course in ",
            "terms of algorithms for the prominent NP-Hard problems, and while needing to prove every claim was several margins tougher than the expectations ",
            "of the basic Algorithm Design course, it was still extremely rewarding trying to piece together simple-but-effective solutions to a host ",
            "of very engaging exercises. "
          ]
        }
      ],
      "Y4S1": [
        {
          "code": "CS3210",
          "title": "Parallel Computing",
          "generalreview": "",
          "thumbnail": "https://hpc.llnl.gov/sites/default/files/styles/with_sidebar_1_up/public/parallelProblem.gif?itok=u4OKbOB5",
          "meme": "https://i.programmerhumor.io/2025/03/d1db667d187952a5a156a4ef5a2cf548.png",
          "prologue": [""],
          "para1": [""],
          "para2": [""],
          "para3": [""]
        },
        {
          "code": "CS4248",
          "title": "Natural Language Processing",
          "generalreview": "",
          "thumbnail": "https://www.blumeglobal.com/media/wp-content/uploads/2018/11/NLP-image-scaled.jpg?rnd=133498791419900000",
          "meme": "https://i.programmerhumor.io/2025/03/547be04ea016a275a9fbcc93b422b844.jpeg",
          "prologue": [""],
          "para1": [""],
          "para2": [""],
          "para3": [""]
        }
      ]
    }
  ],
  "Problems": {
    "TwoSum": {
      "title": "Two Sum",
      "topics": ["array", "hashmap"],
      "difficulty": "EASY",
      "languages": ["PYTHON", "JAVA"],
      "description": [
        "Given an array of integers and a target, determine if there are 2 integers at distinct indices that add up to the target and return their indices. ",
        "You may assume there is exactly 1 unique solution per input."
      ],
      "example": [
        "Input: List - [2, 7, 4, 3, 1] with Target 9. Expected Output: [0, 1]"
      ],
      "solution": [
        "The naive solution to this problem is to check every pair of numbers to see if any sum up to our target. Obviously, this is rather inefficient since for ",
        "10 numbers we would be doing a 100 operations and 10000 for a 100 (i.e. we would be performing n^2 operations for an array of size n). Hence, let's use a ",
        "simple algorithm that can only needs to check n numbers to output a result (i.e. 1-pass through the array). The idea is: we just need to record the difference ",
        "between our target and each member of the array, and check if that difference (i.e. the initial number's 'complement') also exists in the array as a member. Using ",
        "the above example, we would take 2 and create a mapping (7 : 0) in our table. Why 0? Simply because the question wants us to track the indices of the desirable pair, ",
        "so we record 0 for 2. Then, when we get to the next number i.e. 7, we check if 7 could've been combined with a ",
        "previous number to make 9. And we know it can! Since we just recorded 7 for 2. Now, we simply extract the index of the other number (0 for 2 in this case) and return ",
        "the current index (i.e. 1, for 7) with the retrieved index (0) and return [0, 1]. Fin."
      ],
      "analysis": [
        "The reason this solution works is because of the use of 'maps'. We record the exact balance each number needs to sum up to the target, and then terminate as soon as we find ",
        "a match (which can be checked for quickly due to the efficient indexing that maps provide). If we manage to get through the whole array without a single match, we know ",
        "for sure that no number can meet its target using only other numbers in the array. Put another way: we know that no number in the array can be added to another ",
        "to reach the target value. Checking through all possible pairs of values is simply unnecessary for us to conclude that a particular number will never be useful in meeting ",
        "the given target."
      ],
      "image": "https://strapi-iio.s3.us-west-2.amazonaws.com/two_sum_1_3cf64db019.png"
    },
    "MergeTwoSortedLists": {
      "title": "Merge Two Sorted Lists",
      "topics": ["linked list"],
      "difficulty": "EASY",
      "languages": ["JAVA"],
      "description": [
        "Given two linked lists sorted in ascending order, merge all the elements of the two lists into one linked list while preserving the order."
      ],
      "example": [
        "Input: LList 1, (1 -> 2 - > 4) and LList 2, (1 -> 3 -> 4). Expected Output: LList 3, (1 -> 1 -> 2 -> 3 -> 4 -> 4)."
      ],
      "solution": [
        "This is actually simpler than TwoSum. It's just a matter of traversing through both lists, comparing the heads of the lists at each value and slotting the smaller ",
        "value into the new list until both of the lists are exhausted. The only nuance to handle is: what if the two lists are not of equal length? In this case, we ",
        "just have to stop looking at the shorter list once its done, and just append the remaining whole of the longer list to the result list (which we can do since we are ",
        "assured that both lists are fully sorted. Additionally, why do we need ",
        "to maintain 2 lists 'curr' and 'result'? This is because to build a linked list iteratively, we need to keep shifting our list pointer until it reaches the tail. But ",
        "the solution demands the head of the resulting list. So, either we build only one list and spend extra time at the end shifting our pointer in reverse until it gets back ",
        "to the head. Or we can just use 2 lists, and fix one list at the head while using the other list to store the desired final values before appending the second ",
        "list to the first, and then returning the first. Fin."
      ],
      "analysis": [],
      "image": "https://favtutor.com/articles/wp-content/uploads/2023/12/Merge-two-sorted-linked-lists.png"
    },
    "BestTimeToBuyAndSellStock": {
      "title": "Best Time To Buy And Sell Stock",
      "topics": ["array", "dynamic programming"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given a list of prices of a given stock from Day 1 to Day N, determine the maximum profit that can be ",
        "earned from buying and then selling the stock (with a profit of 0 if the stock price is only decreasing)."
      ],
      "example": [
        "Input: List - [7, 1, 5, 3, 6, 4]. Expected max profit: 5 (buy on day 2, sell on day 5)."
      ],
      "solution": [
        "Simply put, to maximise our profit we need to buy at the minimum price and sell at the maximum price. ",
        "A simplistic way to do this would be to compare all pairs of prices with another and take the pair with the highest ",
        "difference as long as the day we are buying is before the day we are selling. However, we can be a little smarter by realising ",
        "that each daily stock price can either be a new minimum or a new maximum i.e. we could just assume the first price we see is the ",
        "lowest we can get and then hope that either (1) we encounter a higher price later on so we may sell for some non-zero profit or ",
        "(2) encounter a lower price than the current minimum so that we can buy lower (i.e. at a better price). Notice however, that the usefulness of (2) is ",
        "contingent on (1) i.e. that the lower buy-price we get can still be later sold at a price that nets us some profit. If the stock price ",
        "is only decreasing, then case (1) will never happen (because there will never be a higher price than the first), and consequently case (2) ",
        "will not be of any use (because the price hits a new low everyday, with no opportunity to sell for a profit). "
      ],
      "analysis": [
        "This is a very simple example of what is called a 'greedy' algorithm because of its linearity, and the only aim being to maximise profit at the lowest price ",
        "we could get. The linearity of the algorithm is also assured by the linearity of time, in that we must buy before we sell (hence, we ",
        "just need to consider each day's price and its potential to maximise profit by being either a new low, or a new high)."
      ],
      "image": "https://www.ggorantala.dev/content/images/size/w1460/2024/02/Best-time-to-buy-and-sell-stock-1.png"
    },
    "LinkedListCycle": {
      "title": "Linked List Cycle",
      "topics": ["linked list", "two pointers"],
      "difficulty": "EASY",
      "languages": ["C"],
      "description": [
        "Determine if a given linked list contains a cycle (i.e. there is some node in the list that can be reached twice ",
        "by continuously following the links between each pair of nodes consecutively)."
      ],
      "example": [
        "Input: LList (3 -> 2 -> 0 -> 4 -> 2). Expected answer: True, because the last node (4) leads back to the 2nd ",
        "node (2), and the cycle continues."
      ],
      "solution": [
        "We will use what is called the Hare-and-Tortoise or Fast-and-Slow pointer approach. The basic idea is simple: ",
        "imagine setting off 2 runners at the same start point, and checking if one runner is able to lap the other ",
        "before the race is over. Intuitively, if the faster runner reaches the endpoint without passing the slower runner ",
        "again (or in fact, reaches the end at all), this means there is never a repetition of nodes i.e. a point where the ",
        "faster runner crosses the slower one again. ",
        "The translation to code from this analogy is almost exact: initialise both pointers at the head of the list, and ",
        "always keep 1 pointer 2 steps ahead of the other until either both meet again (in which case there is a cycle) or ",
        "the advanced pointer reaches the end of the list (in which case, by definition there is no cycle)."
      ],
      "analysis": [
        "An interesting question to ask is: how to determine exactly how much faster the faster runner should be? In the above ",
        "code for example, why is the faster pointer kept 2 nodes ahead instead of 1, or 3? Well, really the requirement is to ",
        "prevent 2 situations: 'chasing-the-tail' (in which both pointers are perpetually one node apart from one another), ",
        "and 'teleporting' (in which the faster pointer is frequently overshooting the slower pointer). It should be obvious ",
        "that the first situation is a result of the faster pointer not being fast enough (never closing the distance to the ",
        "slower pointer), and the second is the opposite. Thus, if we maintain the slower node at a pace of 1 node per step, ",
        "and the faster node at a pace of 2 nodes per step, then the faster node would logically be steadily ",
        "closing the distance between them by 1 node at a time, and it would take exactly d steps to close an ",
        "initial distance of d nodes. Now, we can probably still trial/error a ratio other than 1:2 that would still work, but let's ",
        "keep it simple, stupid."
      ],
      "image": "https://static.takeuforward.org/content/starting-of-loop-image8-jG7vA1Si"
    },
    "BalancedBinaryTree": {
      "title": "Balanced Binary Tree",
      "topics": ["binary tree", "depth-first search"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "A binary tree is height-balanced if the heights of the two subnodes of each node never differ by more than 1. ",
        "The height of a node is equal to the length of the longest path of sub-nodes from the node (inclusive), ",
        "to a leaf. Given any binary tree, assess if the tree is height-balanced or not."
      ],
      "example": [
        "Input: BinTree (in level-order): [1, 2, 3, 4, 5, null, null, 6, 7]. Expected answer: False, because the left ",
        "subnode (2) of the root (1) has height 3 (2 -> 4 -> 6 -> null) whereas the right subnode (3) has height 1 (3 -> null) "
      ],
      "solution": [
        "Let us adapt Depth-First Search (DFS; a form of search that explores each node to its complete depth/terminus before ",
        "moving on to a different node) to solve this problem. As per regular DFS, we use a stack and a visited set to explore ",
        "the tree's nodes. The modification would then be to also calculate the heights of each node as we explore. But how do we ",
        "go about doing this? Quite simply, we start counting the height of each node in a \"bottom-up\" fashion i.e. once ",
        "we reach a leaf, we mark its height as 1 and then as we revisit each of its predecesors, we record the consolidated height ",
        "for each of them based on the max (because height is defined as the \"longest\" path from node to bottom) heights of the successors'",
        "whose heights we've just calculated (this is where the bottom-up nature of DFS helps). If, at any point of our bottom-up traversal, we've ",
        "observed that any node is unbalanced in height - i.e. the consolidated heights of its successors/subnodes differ by ",
        "more than 1 - we can immediately conclude that the tree is unbalanced. In any other case, we simply finish calculating the ",
        "heights of all the nodes in the tree (with the final one being the height of the root node), and then conclude that the ",
        "tree is balanced."
      ],
      "analysis": [
        "This is an iterative (i.e. bottom-up) solution. The recursive (i.e. top-down solution) is easier in terms of code-complexity, but may be more expensive ",
        "in terms of the space required to maintain the call stack (i.e. each recursive invocation & its parameters etc.). In fact, the ",
        "iterative solution - while lengthier code-wise - is (as mentioned) mostly boilerplate  DFS code, with the simple modification ",
        "of explicitly consolidating the height of each node instead of relying on the program's call stack to do it. The \"base-case\" for this solution is the ",
        "same as for the recursive solution, which is either if the given tree is empty (in which case it is trivially balanced), or if we've ",
        "bottomed-out on calculating the depth of any node (i.e. the node has no further subnodes that can contribute to its height)."
      ],
      "image": "https://media.geeksforgeeks.org/wp-content/uploads/20240805164549/balance-vs-unbalance-binnary-tree.webp"
    },
    "ReverseLinkedList": {
      "title": "Reverse Linked List",
      "topics": ["linked list"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given a linked list, rearranged the nodes of the list in reversed order."
      ],
      "example": [
        "Input: LList (1 -> 2 -> 3 -> 4 -> 5 -> º) becomes (5 -> 4 -> 3 -> 2 -> 1 -> º) where º marks the end of the list, i.e. ",
        "the presence of no further nodes."
      ],
      "solution": [
        "This is a simple problem that makes somewhat creative uses of iterative & recursive approaches. The iterative, ",
        "approach in this case is, in my view, easier to grasp: start with the the first node, \"push it all the way to the back\", and then  ",
        "sequentially swap the latter nodes to the front. Using the above example, we start with (1) and º, and create (1 -> º), after ",
        "which we connect 2 to (1 -> º) to create (2 -> 1 -> º), and so on until we have 5 in hand, and the list we've constructed up till ",
        "then is (4 -> 3 -> 2 -> 1 -> º) which leaves the only remaining step to set (5 -> 4 -> 3 -> 2 -> 1 -> º). The process here comes ",
        "down to 4 steps: (1) note down what the neighbor of the current node is, (2) switch the current node's neighbor (->) to point to the newly ",
        "reversed list up to that node, (3) set the head of the list to the (newly connected) current node, and finally (4) move onto the next ",
        "node to be swapped to the front. Obviously, this would continue until we've reached the end of the original list, since at that ",
        "point we should've reversed all the nodes from first to last. The recursive solution, while not a departure from the ",
        "basics of any recursive solution (having base & inductive cases), is a little trickier to spot. The idea is still the same however: ",
        "successively swap the nodes from end to start to reverse the list. Reusing the same ",
        "example list, we traverse to the end (5), then step back to (4) to create (5 -> 4), then to (3) to create (5 -> 4 -> 3), and so on. The ",
        "only real difference between the two solutions remains that the iterative solution is bottom-up (i.e. we start with 1 -> º, then ",
        "2 -> 1 -> º...) whereas the recursive one is top-down (i.e. we start with 5, then 5 -> 4, then 5 -> 4 -> 3...)"
      ],
      "analysis": [
        "I expect this problem emphasises that the fundamentals of recursive & iterative solutions don't change all too much. ",
        "When writing a recursive solution, we start with the base case (in this case, if the list is empty or only contains one node), ",
        "and then construct the inductive case using wishful thinking (in this case - assuming we already have a fully reversed list for ",
        "every other node except the first two, and then just swapping those particular two manually - a step which if applied for every pair of nodes ",
        "recursively, will eventually result in the actual fully reversed list. When writing an iterative solution, we start with the same base case, ",
        "and then traverse through the list in-order while swapping one node's position at a time such that by the start and end of 1 loop, ",
        "we know that exactly one more node has been brought to the front of our list thus far."
      ],
      "image": "https://www.boardinfinity.com/blog/content/images/2022/10/reverse-a-linked-list-image-cover.png"
    },
    "MajorityElement": {
      "title": "Majority Element",
      "topics": ["array", "math"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given a list of numbers, identify the majority element i.e. the number that appears at least half as many times as there are ",
        "numbers in the list. It is guaranteed that there is at least 1 majority element in the list."
      ],
      "example": [
        "Input: List - for [1, 1, 1, 2, 2], the majority element is 1."
      ],
      "solution": [
        "The obvious solution would be to simply go through the list and maintain the frequency of each type of number. However, is there a ",
        "way to solve this problem without needing the extra space to maintain the count of each number? Fortunately, yes! The obvious observation ",
        "to make is that for a number to be a majority element, it has to occupy more space than is occupied by numbers that are not that ",
        "number. So really, all that has to be done is to maintain a counter, and check which number \"survives\" the search i.e. occupies ",
        "at least 1 more space in total than any other number in the list. So, using [1, 1, 1, 2, 2] as an example - the counter starts with ",
        "0 and the majority element is assumed to be 1 (as a start). Then, as we get to three 1s, the counter gets to 3 (i.e. we know 1 occupies ",
        "3 spaces in the list) whereas when we see two 2s, the counter is reduced by 1 each time until it sinks to 1 (i.e. we know that 1 only occupies 1 more space than ",
        "any other number(s) in the list. However, since we terminate our search with the counter at 1, and majority element still 1, it is ",
        "good enough for us to conclude that 1 is the majority element. If, instead we had [1, 1, 2, 2, 2] - then we might still start ",
        "with counter = 0 and majority element = 1, but eventually the two 1s (counter = 2) would be tipped by the three 2s and we would ",
        "end with counter = 1 and majority element = 2, and thus learn that the 2s occupy 1 more space than any other number(s) in the list."
      ],
      "analysis": [],
      "image": "https://files.prepinsta.com/2023/03/Majority-Element-In-An-Array-1024x832.webp"
    },
    "CountingBits": {
      "title": "Counting Bits",
      "topics": ["bit manipulation", "dynamic programming"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given an integer n, compute - in a list - how many 1s are present in the binary representation of every number from 0 to n."
      ],
      "example": [
        "Input: For n = 5, the expected output list is [0, 1, 1, 2, 1, 2] for 0 <-> 0, 1 <-> 1, 2 <-> 10, 3 <-> 11, 4 <-> 100, and 5 <-> 101."
      ],
      "solution": [
        "Do we have to compute the binary representation of every single number from 0 to n? That seems like a necessity at first in ",
        "order to know how many 1s are in each number. But in fact it isn't necessary, and not needing to do that actually makes our solution ",
        "so much easier - we use the power of bit manipulation, and dynamic programming. First, is it obvious that the number of 1s in a ",
        "number X, is nearly the same as the number of 1s in X/2 except for a possible 1-bit difference? In general, the number of bits ",
        "in a number and twice of the same number, differs by only 1 bit - and that bit is either 1, or 0 (obviously); the implication of this is that there ",
        "could only be a difference of 1 in the number of 1s between X and X/2. Further, we can even determine whether the extra bit is a 0 or 1, by checking ",
        "the parity of the number. If its odd, then the extra bit would be 1, otherwise it would be 0; notice the difference between 2 (10), ",
        "4 (100), and 5 (101). This is how bit manipulation is a big help, since using the relation between the binary representations ",
        "of a number and half of itself - we can form the rule that the number of 1s in a number is equal to the number of 1s in half of itself ",
        "+ an additional 1 if the given number is odd (and thus the extra bit is 1). However, this rule is obviously recursive, since to know the ",
        "number of 1s in a number, one would need the number of 1s in half of that number, and so on & so forth. This is what now makes this a ",
        "dynamic programming problem - wherein we start with a base case (0 <-> 0) and use the info we got from each number to construct ",
        "the results for subsequent numbers."
      ],
      "analysis": [],
      "image": "https://blog.kakaocdn.net/dn/2omqt/btrFIeDHAp6/YxZ7ajJC1xSMZE220A8Fuk/img.jpg"
    },
    "LongestCommonPrefix": {
      "title": "Longest Common Prefix",
      "topics": ["string", "sorting"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given a list of strings, determine their longest common prefix."
      ],
      "example": [
        "Input: For list [\"flight\", \"flower\", \"flow\"], the longest common prefix is \"fl\"."
      ],
      "solution": [
        "To determine the longest common prefix, we need to check 2 things: (1) that all strings include a common group of characters (which would later form ",
        "the prefix), and (2) that these characters appear in the same contiguous positions in all strings. The 2nd requirement is easy to fulfil - ",
        "just comparing strings sequentially (i.e. in-parallel at the same positions/indices) would do. ",
        "But how to fulfil the 1st requirement in a relatively efficient & simple manner? This is where sorting helps us! To start, we sort ",
        "all of the strings in lexicographical order (the order in the above example would be [\"flight\", \"flow\", \"flower\"]. Next, we ",
        "compare the smallest and largest strings, and return their common prefix (if any), and then we are actually done! By sorting, we've reduced our ",
        "domain of comparison from all strings, to just 2. But why does this even work? Well, let's think of 2 cases. The first is ",
        "that there are no characters in common between the first and last string - in which case, the longest common prefix between all strings ",
        "is trivially empty. The second is that there is a character x that is common between the first and last string at position i. Is it possible that the ",
        "first and last string have x in common at i but no other string does? Well, no because this is where sorting comes into play. ",
        "If the first and last string have a character x in common at position i that no string in-between does, this means the first string is actually not the ",
        "smallest string. Why is this the case? Suppose the character in position i for a string in-between is y. Since x is present at i for ",
        "the largest string, this means x > y. This means that any string with x in it at position i, must come after all strings with y (or smaller) ",
        "characters at position i - which obviously makes the smallest string, not in-fact the smallest. This is of course a contradiction since ",
        "we know we already sorted the list lexicographically. Think of an example: [\"ad\", \"ab\", \"ac\", \"ad\"]. The first and last strings seem to share ",
        "a character \"d\" in the 2nd position that no string in-between does, but this list is not even lexicographically sorted. If it were, ",
        "then the first string would be \"ab\", and the longest common prefix would be correctly identified as \"a\". Obviously, once the first ",
        "and last string are mismatched (in this case, at \"b\" and \"d\", we can return the prefix we have so far (since again, a prefix has to be contiguous ",
        "and once the chain is broken anywhere the prefix ends there too."
      ],
      "analysis": [
        "This solution may not necessarily be more efficient than more brute-force solutions (such as comparing the characters of all ",
        "strings in every position). However, it is definitely much simpler in-code, and not necessarily much worse in performance. Debugging ",
        "and testing also tends to be easier with simpler solutions - which is a genuine consideration!"
      ],
      "image": "https://www.interviewbit.com/blog/wp-content/uploads/2021/10/Horizantal-Scanning-Approach.png"
    },
    "MissingNumber": {
      "title": "Missing Number",
      "topics": ["array", "bit manipulation", "math"],
      "difficulty": "EASY",
      "languages": ["PYTHON"],
      "description": [
        "Given an array of n integers, there will be 1 integer from 0 to n (inclusive) that is missing. Find this number."
      ],
      "example": [
        "Input: For list [3, 0, 2] of 3 numbers, the missing number between 0 and 3, is 1."
      ],
      "solution": [
        "Four ways to solve this problem. The first is trivial: sort the list, and then identify the missing number at the point where there is a ",
        "is a mismatch between the index of a number and the number itself (for the above case, we sort the list to get [0, 2, 3], and ",
        "when we see 2 occupying index 1 instead of 1, we know that 1 is missing). The second is equally trivial: create a separate list of integers ",
        "from 0 to n, and then just check individually that each number is present in the original list. While these two methods solve the ",
        "problem for us, they do not do so very efficiently (taking up extra time & space respectively, than needed). The third method, is therefore an ",
        "optimisation which exploits the idea of sums. If we sum up the numbers from 0, 1, ..., n, and then take the difference between this ",
        "and the sum of the numbers in the given list, would we not automatically find the missing number? Since 0 + 1 + 2 + 3 = 6 - (0 + 2 + 3) = 1. ",
        "In fact, we don't even need to calculate the sum of 0 to n by-hand, if we just use the sum formula i.e. n(n + 1) / 2, which would instantly ",
        "give us 6. While this solution is theoretically good enough, given that it uses no extra time or space ",
        "than is just required to sum up the n - 1 numbers in the given list, it is exposed to the risk of integer overflow if we're working with ",
        "summing very large numbers. The 4th solution, is thus an enhancement - in terms of safety - by using bit manipulation. The key idea is that ",
        "the XOR (Exclusive Or) of a number with itself, is 0 while the XOR of a number with 0, is the number itself. Why is this useful? Well, ",
        "if we compare the list of 0 to n and the list of given numbers side-by-side, we realise that there will necessarily be exactly 2 copies of ",
        "each number except the missing one (i.e. 0-0, 1, 2-2, 3-3). What we can then do, is XOR all the numbers together (i.e. 0 ^ 0 ^ 1 ^ 2 ^ 2 ^ 3 ^ 3) ",
        "and we would end up with (0 ^ 1 ^ 0 ^ 0), which is 1. How neat!"
      ],
      "analysis": [
        "Apart from using basic properties of XOR (of XORing a number with itself, and a number with 0), the key utility of XOR ",
        "is that, like addition & multiplication, it is commutative (i.e. a ^ b = b ^ a), and associative (i.e. (a ^ b) ^ c = a ^ (b ^ c)) - both properties which can be proven ",
        "easily using truth tables or boolean algebra. This allows us to directly XOR all the numbers together without needing to group them first. ",
        "I.e. if we're given [3, 0, 2] as our list - we can directly do 0 ^ 1 ^ 2 ^ 3 ^ 3 ^ 0 ^ 2 to get 1, without needing to actually sort and ",
        "arrange our XOR to execute exactly 0 ^ 0 ^ 1 ^ 2 ^ 2 ^ 3 ^ 3."
      ],
      "image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqOO2T5Q106XzjBS6CLt1uyzOHjgqZ4UaqaVNfgYivJB8PN99-ZjU7-fZMK9UQSNb9q_MNv74DTX8_oaDqawhSK_wMuW5ORQ2pV6gJt__SfAi3I1VRVfxQ4K4nRt48JzpB3uPQxeQhSI5D/s1600/2.png"
    },
    "MaximumSubarray": {
      "title": "Maximum Subarray",
      "difficulty": "MEDIUM",
      "topics": ["array", "dynamic programming"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of integers, find the maximum sum value of the numbers in any subarray of the original array."
      ],
      "example": [
        "Input: For array [-2, 1, -3, 4, -1, 2, 1, -5, 4], the subarray [4, -1, 2, 1] has the largest sum 6."
      ],
      "solution": [
        "This is a typical dynamic programming problem since it involves optimising over some unknown number of combinations of a variable-size input. The question becomes - how to converge ",
        "efficiently on the maximum subarray sum without needing to compare all possible subarrays. Intuitive idea: imagine starting off our search at the first number of the array, ",
        "and adding each subsequent number into the sum so long as the number is able to contribute positively to the sum at hand. If it is the case that out of all the numbers we're adding, ",
        "just keeping the last one is somehow greater than taking it together with the whole sum, then we will do just that. Using the above example, we start with a sum of -2 ",
        "(it being the first number), and then consider the next number, 1. Since -2 + 1 = -1 < 1, we realise that -2 is actually useless to us since we are better off having a sum consisting of just 1. ",
        "In other words, the subarray [1] provides a greater sum than the subarray [-2, 1] - which leads us to eliminate [-2, 1] as a possible answer and consider [1] as the current best answer. ",
        "We then get to -3 and we see that 1 + (-3) = -2 > -3. In this case, -3 is not better than -2, but neither is -2 better than 1. Thus, while we don't yet ditch this subarray ([1, -3]) at -2, we still ",
        "retain our maximum sum thus far as 1. That is, we keep expanding the subarray [1, -3, ...] in hopes of it eventually picking up. But if we can't ever sum to something greater than 1, ",
        "then we conclude that 1 was actually the best we can get. Luckily for us though, once we get to to 4, we see 1 + (-3) + 4 = 2 < 4, and our summation restarts again from 4 with our new ",
        "maximum becoming 4. Why? Again, we realise that we are better off with a sum consisting of just 4, instead of 1 + (-3) + 4. Eventually, we reach [4, -1, 2, 1] (= 6) as the new maximum ",
        "before we encounter the -5 and 4 that winds our sum down to 6 - 5 + 4 = 5, which is no better than 6. As a result, we keep 6 in the end. And 6 is indeed, the best we can do!"
      ],
      "analysis": [
        "What does this problem require? 2 things - a \"local\" maximum, and a \"global\" maximum. Our local maximum ensures that our summation is \"productive\" by signalling a \"sum restart\" ",
        "when necessary (i.e. a signal to check the next subarray already since the current subarray will definitely not get more optimal by adding more consecutive numbers). ",
        "Our global maximum ensures our summation is \"optimal\" by comparing successive \"productive\" subarray sums, and retaining the bigger one each time - till we've scanned the whole array. ",
        "This solution thus stays linear in runtime (i.e. only requires 1-pass through the whole array) by exploiting the nature of subarrays as contiguous (we can safely \"restart\" the summation ",
        "once a consecutive number is better - instead of worrying about a combination of disjoint numbers across the array), and by using local and global maximums to store and check maximality."
      ],
      "image": "https://miro.medium.com/v2/resize:fit:676/1*UrQhblF8B-6QoEC6E7kWow.png"
    },
    "InsertInterval": {
      "title": "Insert Interval",
      "difficulty": "MEDIUM",
      "topics": ["array"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of intervals (one interval being in the form of <start, end>) sorted in ascending order of start time, insert a given new interval into the array such that the new interval does not overlap ",
        "with any of the existing intervals, and all the intervals are still sorted. Merge overlapping intervals if necessary."
      ],
      "example": [
        "Input: For array [<1, 3>, <6, 9>], the new interval <2, 5> would be inserted into the array as such: [<1, 3>, <2, 5>, <6, 9>] (ascending order of start time), and then the ",
        "overlapping intervals <1, 3> and <2, 5> would be merged to become <1, 5> thus leaving the new intervals array as [<1, 5>, <6, 9>]. "
      ],
      "solution": [
        "The tasks for this problem are clear - inserting the new interval in the right place, and making sure it does not overlap with any of the other intervals. The difficulty is just ",
        "implementing it in an efficient and simple manner. Let's break it down into 3 steps. First, identify all the intervals that end before the new interval starts. These are the first ",
        "\"half\"of intervals - the ones which come before the new interval chronologically, and do not overlap with it. Second, identify the intervals that start after the new interval ends. ",
        "These are the \"back-half\" intervals - the ones which come after the new interval chronlogically, and do not overlap with it. The third step is the \"tricky\" part - the middle intervals ",
        "that do overlap with the new interval (if any). Notice that regardless of the nature of the overlap (full or partial), we just need to adjust the new ",
        "interval to resolve the overlap. We can do this by comparing the new interval with each overlapping interval, and creating a new interval that covers both (what the question calls: \"merging\"). ",
        "For example, in the above example, <1, 3> and <2, 5> have a partial overlap since <2, 5> starts in between <1, 3>. To create the merged interval, we compare 1 and 2 and retain the start ",
        "point as 1 (since 1 < 2), and compare 3 and 5 and retain the end point as 5 (since 5 > 3). The non-overlapping (merged) interval is thus <1, 5>. Similarly, if we have an overlap such as ",
        "<1, 5> and <2, 3> where the new interval is fully subsumed within an old interval, then we retain the merged interval as <1, 5> since its start & end points cover both intervals. Alternatively ",
        "if we have an overlap such as <2, 5> and new interval <1, 3>, we take 1 from <1, 3> and 5 from <2, 5> and create <1, 5> as the merged interval. Basically, we merge overlapping intervals ",
        "by adjusting the new interval to cover all the endpoints of all its overlapping intervals such all ranges are preserved."
      ],
      "analysis": [],
      "image": "https://learn.innoskrit.in/content/images/2023/12/4-2.png"
    },
    "ThreeSum": {
      "title": "3Sum",
      "difficulty": "MEDIUM",
      "topics": ["array", "two pointers"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of integers, find all distinct triplets of integers which each sum to 0 when adding the 3 integers in the triplet together."
      ],
      "example": [
        "Input: For array [-1, 0, 1, 2, -1, -4], all possible triplets are [-1, 0, 1], [-1, -1, 2], and [0, 1, -1]. So, the expected output is [[-1, 0, 1], [-1, -1, 2]] since [-1, 0, 1] and ",
        "[0, 1, -1] are indistinct so we need only retain one."
      ],
      "solution": [
        "In theory, the solution for this is simple. To make the search for \"0-triplets\" easier, we just take each number in the list, and check if there are 2 other numbers which if added ",
        "to the number at hand, would result in 0. That is, instead of permuting over 3 numbers, we fix 1 number at a time and permute over the other 2 instead. How to do this efficiently though, i.e. either find the other 2 ",
        "integers quickly or quickly conclude that the number at hand can't be in any 0-triplet? This is where sorting the array helps us, for a similar reason as it helps in binary search. ",
        "In short: if a possible triplet would lead to a sum that is greater than 0, it means we don't need to consider pairs with any numbers greater than these as the sum of these ",
        "triplets would be even greater. Equivalently, if a possible triplet would lead to a sum that is smaller than 0, it means we don't need to consider pairs with any numbers smaller ",
        "than these, as the resulting triplet-sum would be even smaller. The last thing to check for, is making sure we avoid duplicate triplets. Again, since our array is sorted, this is easy ",
        "since duplicate numbers would be grouped together and if we notice that if a consecutive number is one that we just finished forming a triplet for - we can simply skip it. As usual, let's do ",
        "an example run-through, using the array [-1, 0, 1, 2, -1, -4]. First, we sort this to get [-4, -1, -1, 0, 1, 2]. Then, in the first round we fix -4 as the number and try to find 2 numbers, x and y, ",
        "that sum to 4 such that -4 + x + y would be 0. We check for -1 + 2 which is 1, and which is smaller than 4 - indicating that we should use a bigger left operand to try to increase our sum. ",
        "We then try 0 + 2 = 2, which being still not enough, we try 1 + 2 = 3. By the next round, we are only left with 2, and since we can't use the same number more than once, we're out of luck with finding 2 distinct ",
        "numbers to complement -4 to get 0, and thus we eliminate -4 as possibly being part of any triplet. Notice the \"left\" movement here. We exploit the fact that the numbers are listed ",
        "in ascending order and thus if we want our sum to get bigger, we simply fix our right operand (the bigger number), and keep shifting our left operand from left to right (increasing values). ",
        "Obviously, if we wanted our sum to get smaller, we would instead fix our left operand (the smaller number) and keep shifting our right operand from right to left (decreasing values). ",
        "In the next round, we fix -1 and repeat the same process (i.e. find 2 numbers which when added to -1, would result in 0). We try -1 + 2 = 1 at first, which already gives us 1. Thus, our first triplet becomes [-1, -1, 2]. Now, we check for other possible pairs ",
        "that could complement -1 to form 0. We try 0 + 1 = 1, which fortunately again is exactly what we want. So, our second triplet becomes [-1, 0, 1]. At this point, we've exhausted the search for -1 and ",
        "move onto the next number in the array which is also -1. As mentioned, we skip this -1 since we just covered -1, and any triplets we discover now would consist of the same numbers. In this way, ",
        "we carry on until we've exhausted the triplet-search for all numbers in the array."
      ],
      "analysis": [
        "This solution is not extremely efficient - coming in at O(n^2) due to checking roughly (n/2) + (n - 1)/2 + (n - 2)/2 + (n - 3)/2...values in total. But in general, it is a decent ",
        "optimisation over the O(n^3) brute force checking of all possible triplets in the array. The code above also includes a small but additional optimisation by discarding checking values ",
        "that are greater than 0, since any values further than this point would obviously only lead to a sum greater than 0 (remember, the array is sorted in non-decreasing order). So for example, with an array like ",
        "[-1, 0, 1, 2, 3, 4], we would get the triplet [-1, 0, 1] and then stop checking for triplets after 0 since no possible triplet amongst [1, 2, 3, 4] would ever have a sum of 0. This, ",
        "does not change the big-O complexity of our solution since in the worst case, all or most of our numbers would be <= 0 (like in the initial example [-4, -1, -1, 0, 1, 2]), in which case ",
        "we cannot discard most of the array. But on average, we may be able to discard half the array if there are an equal number of positive & negative integers, reducing our work by half again to ",
        "1/4(n + n - 1 + n - 2 + ...)."
      ],
      "image": "https://miro.medium.com/v2/resize:fit:1400/1*BysOt8xlCGUSZqZ1y9i4NQ.png"
    },
    "ProductOfArrayExceptSelf": {
      "title": "Product of Array / Self",
      "difficulty": "MEDIUM",
      "topics": ["array"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of integers, at each integer - calculate what the product of all the integers of the array would be, excluding that integer."
      ],
      "example": [
        "Given the array [1, 2, 3, 4], the expected output is [24, 12, 8, 6] i.e. [2 x 3 x 4, 1 x 3 x 4, 1 x 2 x 4, 1 x 2 x 3]."
      ],
      "solution": [
        "It would be trivial to solve this problem if we could calculate the product of the array once, and then simply divide this product at each integer of the array (i.e. calculate ",
        "1 x 2 x 3 x 4 = 24, and then return [24 / 1, 24 / 2, 24 / 3, 24 / 4]. So let's assume we're not allowed to use the division operation. Further, let's also assume that we must find a ",
        "more efficient solution than simply re-calculating the product n times and excluding 1 integer every time (which would be in the order of n^2). These constraints spice things up a bit, but it seems the ",
        "solution is still fortunately quite simple if we just use the concept of prefix & suffix products! For context, a \"prefix product\" is a multiplication of a sequence of integers up to and ",
        "including each integer in the list. For example, the prefix product sequence of [1, 2, 3, 4] would be [1, 2, 6, 24] i.e. [1, 1 x 2, 1 x 2 x 3, 1 x 2 x 3 x 4]. It is a sort of \"precomputation\" ",
        "tool that helps us keep track of the running product over an increasing subsequence of the array. A suffix product can be analogously understood (the suffix product sequence of [1, 2, 3, 4] ",
        "would be [24, 24, 12, 4], for example). How does this help us? Well, we can think of the requested array in the problem to be a combination of prefix-and-suffix product. When calculating ",
        "the products at the start for example, the suffix product of the integers at the back is more relevant, while for the products at the back, the prefix product of the beginning integers is useful. ",
        "To show how the combination is done, let us use the example of [1, 2, 3, 4]. First, we build the prefix array (but instead of multiplying till 4, we just calculate the prefix product of 1, 2, 3). ",
        "We now have [1, 1, 2, 6] as our prefix product array ([1, 1, 1 x 2, 1 x 2 x 3]). Why did we not include 4? Remember that for the final output array, the final element should be the product of the ",
        "array without 4 (i.e. 1 x 2 x 3). Now, we use the same array to combine our suffix products, and the resulting array synthesized should be the expected output array. Specifically, ",
        "with [1, 1, 2, 6], we work backwards from the 2nd last integer (again, since 6 is already correct for the final output), and multiply each integer with the suffix product at the previous stage. So ",
        "for 2, i.e. the 3rd integer in the prefix array, we multiply the suffix product at the 4th integer - i.e. 4 in this case - since the 3rd integer is the product of the first 2 integers of the array ",
        "and the only thing missing is the 4th integer (with the 3rd integer of the original array to be excluded). Moving on, for the 2nd integer in the prefix array, we multiply the suffix product ",
        "of the the 4th and 3rd integers of the original array - i.e. 4 x 3 = 12 since the 2nd integer of the prefix array is the first integer of the original array, and needs to be multiplied ",
        "with the 3rd and 4th integers of the original array (i.e. exclude the 2nd integer of the original array) to form the expected output."
      ],
      "analysis": [
        "This solution is unconventional, and not very easy to think of. The first part is figuring out how to know to bring prefix & suffix products into this. To me, its the notion of ",
        "\"running products\" that is the hint. Once we construct the running products from start to end (not including the end), and from end to start, we need only to combine them in a ",
        "way that cleverly excludes exactly the integers we want to exclude (which again, should be easier because prefix & suffix products are already a form of combining elements in subsequences ",
        "instead of all at once. Figuring out how exactly to combine the suffix products into the prefix product array is then, the next trick. ",
        "To realise how to do this, perhaps we can think of how to match the \"places\" of the integers in the prefix product array to the integers that are supposed to ",
        "form their product in the final array. For example, we know the 3rd integer of the final array should be the product of the 1st, 2nd, and 4th integers. In the prefix array, we have the ",
        "3rd integer as the product of the first 2 integers - all that is missing is the 4th. Similarly, we know the 2nd integer of the final array should be the product of the 1st, 3rd, and ",
        "4th integers, and in the prefix array it is the 1st integer - what is missing is the 3rd and 4th. Hence, we know that we should be increasing the suffix sum as we go down the prefix ",
        "array and multiplying the prefix products. And more importantly, each integer should be multiplied by the suffix product of the previous integers."
      ],
      "image": "https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8y9wb620m4n7kakyj5rn.png"
    },
    "RottingOranges": {
      "title": "RottingOranges",
      "difficulty": "MEDIUM",
      "topics": ["array"],
      "languages": ["PYTHON"],
      "description": [
        "You are given a grid (i.e. 2D array) of integers - either 0, 1, or 2. 0 represents an empty cell, 1 represents a fresh orange, and 2 represents a rotting orange. You are told ",
        "that at any 1 time, a single rotting orange can rot any fresh oranges in its 4-dimensional (i.e. one cell up/down, or one cell right/left) vicinity. Those rotting oranges can then ",
        "rot the oranges adjacent to them, in the next round. Count the minimum number of rounds it would take for all the oranges in the grid to rot. If this is impossible (i.e. there is some ",
        "orange that is unreachable 4-directionally, and hence cannot be rotted), return -1."
      ],
      "example": [
        "Given the grid [[2, 1, 1], [1, 1, 0], [0, 1, 1]], the expected output is a minimum 4 rounds of rotting for the grid to become [[2, 2, 2], [2, 2, 0], [0, 2, 2]]."
      ],
      "solution": [
        "This problem has 2 requirements: analyse the duration of the quickest rotting process by executing it, and ensure that this rotting process does not run-into duplicates i.e. rotting ",
        "oranges that were already rotted before. Now, what if I told you that you don't have to think too hard about what rotting process would be quickest? Simply imagine that the oranges ",
        "were growing on a tree (as would be their natural place!) instead of represented on a grid. Now imagine the tree start rotting from the top, with the topmost orange rotting the ",
        "oranges on the same stem as it, with those oranges rotting oranges connected to their stems. In this way, the rot spreads from top to bottom, breadth-wise, with all oranges connected ",
        "by connecting stems (or sets of branches) rotted together by the end. If this sounds familiar, it is indeed just Breadth-First Search but as applied to a grid (or 2D array) instead of ",
        "a graph (or tree). In essence, we garner a queue of all the already rotten oranges in the tree (I shall say this instead of \"grid\"), and treat them as \"roots\" which spread their ",
        "rot breadth-wise to their neighboring branch/stem fresh oranges. We dequeue 1 rotten orange, rot all its fresh neighbors 4-directionally, and then enqueue the newly rotted neighbors so ",
        "they shall spread the good word of their rot to their neighbors (i.e. when these newly rotted oranges are later themselves dequeued). But wait, what about the problem of not adding ",
        "duplicate oranges to the queue such that we are rotting the same oranges over and over again in a spiral? Well, the fix is simple: we first count how many fresh oranges were present at ",
        "the start, and then terminate our search once we've run out of fresh oranges to rot (obviously, we would need to subtract 1 from this count each time we execute a 4-way rot for each ",
        "rotting orange in queue). This way, if we still have rotten oranges in queue but our count of fresh oranges has bottomed out, this is our warning that any enqueued oranges are duplicates ",
        "and cannot possibly be rotting any fresh oranges."
      ],
      "analysis": [
        "This is a pretty basic application of breadth-first search to 2D arrays (the code above is quite simple), but the idea of applying BFS to grids requires some \"daring\" to begin with. ",
        "We can treat the cells of a grid, as the nodes of a graph, and we can treat the neighboring cells of a cell ([i + 1, j], [i - 1, j], [i, j + 1], [i, j - 1]) as the adjacent nodes of a ",
        "node. We simply just to have to be careful about \"invalid\" directions i.e. those that exceed the boundaries of the grid, or those that correspond to empty cells or already rotten oranges. ",
        "Notice that the solution would not actually need to be modified too much even if oranges could rot each other diagonally; we would just be adding a few more directions ",
        "([i + 1, j + 1], [i - 1, j - 1], [i + 1, j - 1], [i + 1, j - 1], ...) in how we enqueue new \"nodes\" (cells, or oranges) to our queue."
      ],
      "image": "https://miro.medium.com/1*bwmno6oBUdtCnQeCp89wOQ.png"
    },
    "CombinationSum": {
      "title": "CombinationSum",
      "difficulty": "MEDIUM",
      "topics": ["array"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of distinct integers and a target sum, find all unique combinations of integers in the array that sum up to the given target. Any integer can be re-used in the same combination ",
        "without limitation, as long as no two combinations consist of the same integers."
      ],
      "example": [
        "Given the array [2, 3, 6, 7] and the target sum 7, the possible combinations are [2, 2, 3], and [7]. Notice that only one of [2, 2, 3], [2, 3, 2], and [3, 2, 2] would be included."
      ],
      "solution": [
        "This problem is a good introduction to a solving technique called \"Backtracking\". Backtracking, in theory, is simple: when a problem requires us to explore some finite collection ",
        "of possibilities over a finite set of elements in search of some target combination(s), then we start small and build our possibilities smartly i.e. by checking if including x element ",
        "in y combination would result in getting closer to the target, and then \"backtracking\" on that possibility if not. So, in this problem in particular - we first consider combinations ",
        "of size 1 (i.e. consisting of one integer: [2], [3], [6], [7]), and then size 2 (i.e. [2, 2], [2, 3]..., [3, 3], [3, 6], ...), and so on. At each level, we check whether the next level ",
        "is worth exploring. For example, we may explore up to [2, 2, 3], and then realise that since the target here is already 7, adding any of the remaining integers (in this case, 6, or 7), ",
        "to the combination [2, 2, 3], would only overshoot 7. Hence, we add [2, 2, 3] to our set of desired combinations, \"backtrack\" from it, and move onto exploring another possible set of ",
        "combinations. Now, notice, that a combination can either grow breadth-wise or depth-wise. Breadth-wise, for example, would be something like [2, 3, 6, 7], whereas depth-wise would be ",
        "[2, 2, 2, 2]. So, essentially what we have to do is explore each integer in-depth (cutting-off when the sum reaches or overshoots the target), and add breadth to each depth-level. ",
        "So for the integer 2, we would explore [2], [2, 2], [2, 2, 2], [2, 2, 2, 2], [2, 3], [2, 2, 3], [2, 2, 2, 3], etc. While this works in rooting out all possible combinations across ",
        "the whole array, we can be smarter in eliminating possibilities both breadth-wise and depth-wise. Again, once we've reached or overshot the target - we can quit adding more numbers. ",
        "So depth-wise, we can stop at [2, 2, 2, 2] since the sum is 8 and we've already overshot 7. Meanwhile breadth-wise, we can stop at [2, 2, 3] instead of trying [2, 2, 3, 6] and [2, 2, 3, 7] ",
        "since [2, 2, 3] already equals 7. However, what is the assurance that adding numbers would necessarily increase the combination sum? After all, it was never stated that the array would ",
        "only consist of positive integers, so something like [2, 2, 3, -1, 1] would also be acceptable. This is where it helps to sort the initial array in ascending order before finding combinations! ",
        "With this step, we can be sure that integers from further \"down the line\" would only increase the existing sum since their values are increasing. One more concern though: what about ",
        "the part where duplicate combinations are disallowed? Well, that's a little bit more of a gimmick in our code - we simply ensure that everytime we begin to construct a new combination sum, ",
        "we do not allow the integer in focus to be paired with integers that we've already considered in the past. Simply put, once we finish all combinations that start with 2, and then move ",
        "onto those that start with 3, we only follow-up with integers that are >= 3. This eliminates adding duplicate combinations like [3, 2, 2] after having added [2, 2, 3] in the previous round - ",
        "i.e. the round where we focused on 2 to construct combinations such as [2, ...]."
      ],
      "analysis": [
        "Backtracking is best visualised in the form of a tree. We start with the \"base case\" - in this case an empty combination [] - and then create our \"levels of depth\" - in this case ",
        "slowly increasing the size of the combinations we're interested in - in this case 1 integer, then 2, then 3, and so on. The branches (or breadth) of the tree would consist ",
        "of the possible combinations themselves at each depth-level - in this case Level 1 would be [2], [3], [6], [7], Level 2 would be [2, 2], [2, 3], ..., [3, 3], [3, 6], and so-on. After ",
        "drawing out the full combination tree, we begin picking our fruits i.e. the desired/target combinations. And if we apply the smart approach, we wouldn't even need to draw the full combination ",
        "tree since we disregard or \"prune\" either branches or depth-levels that would certainly not yield any fruitful results. As an aside, applying pruning improved the runtime performance ",
        "of the above code from 7ms (~84th percentile) to 3ms (~97th percentile)."
      ],
      "image": "https://miro.medium.com/1*f1ZRTjP5tmg6K4gHDnXnlg.png"
    },
    "SearchRotatedSortedArray": {
      "title": "SearchRotatedSortedArray",
      "difficulty": "MEDIUM",
      "topics": ["array"],
      "languages": ["PYTHON"],
      "description": [
        "Given an array of distinct integers sorted in ascending order, it is possible that the array has been \"rotated\" some number of times. Rotated in this context means, for the integers ",
        "in the array to be shifted left or right some k amount of times where k is an integer in the array. The goal is, given some number n where n is an integer in the array, to find the index ",
        "of it - which could have shifted by some factor of k, in the possibly rotated array."
      ],
      "example": [
        "Given the array [0, 1, 2, 4, 5, 6, 7], rotated to become [4, 5, 6, 7, 0, 1, 2] i.e. k = 4, find the new index of the number 0. The expected output would be 4."
      ],
      "solution": [
        "Obviously, we could just conduct a linear search on this array to find the new index of the target number, so let's veto that problem-solving technique and instead task ourselves to ",
        "to find the index within at most log(n) operations. Immediately, binary search comes to mind. How do we utilise binary search to find the new index of the number? It's actually simple ",
        "if we keep in mind where binary search is applicable - only when the list is sorted. Keeping that in mind, let's recognise that in a rotated array, there is a middle index, and either ",
        "the left extreme to the middle index is sorted, or the middle index to the right extreme. In the above example, the middle is 7, and it is the left side [4, 5, 6, 7] that is sorted instead ",
        "of [7, 0, 1, 2]. Thus, what we must do is assess which half is sorted, and execute our binary search on that half to find the target number. Of course, it is a little more complicated than ",
        "that since the array is still rotated, which means our number could be \"hiding\" in the unsorted half. For example again with the above case, though it is [4, 5, 6, 7] that is ",
        "sorted, the target 0 is within [7, 0, 1, 2]. Hence, what we do is set our start point to be middle + 1, and our end point to be the right extreme such that we then explore [0, 1, 2] which ",
        "is still sorted. Now, why can't we just regularly check which range of numbers our target falls into? For example, why can't we just check 4 <= 0 < 7 or 0 <= 0 < 2 and then eliminate ",
        "the invalid range? Well, this does work for the array [4, 5, 6, 7, 0, 1, 2]. But consider the array [5, 1, 3] with target 5. At the start we would check 5 <= 5 < 1 which would be eliminated as invalid ",
        "and well, there already arises a problem. In essence, we could be misled into discarding a possible range if do not first ensure that we are checking a range that is properly ordered for our ",
        "purposes. Our goal in this problem should be narrow down which range of numbers our target falls within, and we can only do this if we are sure that the range we are checking follows ",
        "a monotonically increasing (or decreasing) sequence - the crux of binary search!"
      ],
      "analysis": [],
      "image": "https://read.learnyard.com/content/images/2025/02/Search-in-rotated-sorted-array-leetcode-solution.png"
    }
  }
}

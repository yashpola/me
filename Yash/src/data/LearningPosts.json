{
  "Years": [
    {
      "Y3S1": {
        "Computing": [
          {
            "code": "CS2109S",
            "title": "Introduction to AI/ML",
            "generalreview": "http://disq.us/p/3188fkj",
            "meme": "https://pbs.twimg.com/media/FSDfnEyUcAEUMSd.jpg:large",
            "prologue": [
              "Going in to this course, I had honestly no better clue of how AI/ML ",
              "works than than layman despite the great amount of recent public exposure. Leaving however, I ",
              "had at least an understanding of the basic motivations of how AI/ML is ",
              "exercised. Let me start with a brief recap of the whole course."
            ],
            "para1": [
              "As a simpler exercise: how might an artificial agent answer a simple, factual question? Fortunately, this is easy: ",
              "the agent could reduce the question to a search problem and \"find\" the answer in a deterministic manner. Take this query: ",
              "provide the smallest number of movies that connect Timothée Chalamet and Robert Pattinson (the answer is 2: Timothée acted with Zendaya in Dune, ",
              "and Zendaya with Rob in The Drama). To program an agent to compute this, we clearly first need data supporting the query (in this case, actors and movies). ",
              "Then, we may represent the data in terms of nodes & arcs (example figure below) and simply run the Breadth-First Search algorithm to retrieve our desired path ",
              "(namely, Tim -> Zendaya -> Rob). While the example I used was simple, in essence the task is to create states representing data, define ",
              "transitions between them (which may or may not have variable costs, depending on our problem definition), and define start and end points."
            ],
            "para2": [
              "Now, a more complicated ask: how might an artificial agent make decisions/predictions? This is not so easy, but it comes down to math. ",
              "More generally - we have feature observations (x) which, through some unknown polynomial combination involving unknown weights (w), map ",
              "to outputs (y) which we try to approximate through a hypothesis function that produces predictions (y'). The task is really to find the ",
              "best values for the unknown w, which of-course requires first devising a good hypothesis function (prediction line/polynomial combination). ",
              "What could x and y represent though? Well, likely anything that can be represented as a finite set of attributes over a numerical interval. ",
              "Two examples: (1) predicting house prices (y) based on location, size, and age (x) and (2) predicting whether a tumor is benign or malignant (y) ",
              "based on diameter in cm (x). In this course, I learnt of two broad techniques: classification, and regression. Classification, as the name ",
              "suggests, is the task of labelling/discretizing data while regression (as I learnt it) refers to (usually continuous) aggregation ",
              "over numerical data. For regression tools, I learned of Decision Trees & Linear Regression. For classification, I learned of Logistic ",
              "Regression, Support Vector Machines, Perceptrons, and Neural Networks. While both techniques have their use cases, they may sometimes ",
              "be used in place of other (ML is a very flexible field!). The main difference between these techniques is in their objective functions (used ",
              "to find the best weights (w), and the complexities with which they describe the relationship between x, w, and y (prediction lines). ",
              ""
            ],
            "para3": [
              "At the risk of elaborating too much however, I will comment briefly - before closing - on just one of the topics I found to be the most cool: ",
              "Support Vector Machines (SVMs). First, consider the motivation: we want as confident a classification as possible of training samples x i.e. ",
              "we do not want our model to predict 1 for 0, and 0 for 1 (as an example). Thus, we focus on what is called the \"margin\" (read: distance) ",
              "between the two (I say two because in this course we mostly deal with binary classification) classes of observations and aim to maximise it. ",
              "Of course, this goal is formalised through an objective function (specifically, the Lagrangian quadratic program). I find this cool because (1) I feel ",
              "it really exemplifies the elegance of ML as making creative use of (conceptually) simple algebra to do most of the work, and (2) combines several ",
              "enthralling concepts together (to say the least: calculus, quadratic programming, primal/dual, and algebra). To end off with a quote by the late ",
              "Patrick Winston from MIT: \"You would think that after people have been working on this sort of stuff for 50 or 75 years, there wouldn’t be any ",
              "tricks in the bag left. But that’s when everybody gets surprised.\""
            ]
          },
          {
            "code": "CS3231",
            "title": "Theory of Computation",
            "generalreview": "http://disq.us/p/3188oot",
            "meme": "https://preview.redd.it/51c2i8p9ovy31.png?auto=webp&s=5f1ccf07f0d843b344748912f09d16eb86349691",
            "prologue": [
              "A course unlike any CS course I've done or will ever do. Theory of Computation was all about generalising (rigorously) the computational ",
              "power of the machine. Think: what exactly does it mean for a machine to \"compute\" the answer to a problem? Sure, your Algorithms class ",
              "enumerated and proved to you how certain routines solve certain problems. But how exactly do routines translate in a machine? Probably, your ",
              "Computer Architecture class enlightened you to the data and control paths. Then, your OS class swooped in to give you process abstraction. ",
              "These are obviously crucial, but what lies even further beyond? What is the most general representation of a machine? ",
              "Or indeed, even of a problem that has a specific solution? What exactly can('t) a machine compute? These questions, as (basically) a historical ",
              "review of computational innovation, are devised and answered by the theory of computing. In this recap, I aim - rather ambitiously - to recap the ",
              "the course content in 3 paragraphs, though I acknowledge now that there will be several many concepts I leave out or explain crudely. But perhaps ",
              "the task is just Hard ;)"
            ],
            "para1": [
              "First thing to note: we deal - more generally - in \"languages\" instead of \"problems\", and talk about \"language acceptance\" ",
              "instead of \"problem solving\" (in the traditional/algorithms sense). Simply put: a machine is given a \"string\" as an input ",
              "and is asked to verify the string's membership in a specific language. If the machine \"accepts\" only those strings that belong to ",
              "that language (nothing more or less), then it is said to accept the language as a whole. Further, the language \"of\" the machine is ",
              "said to be that language. Now, for some context: a string is simply an ordered sequence of symbols (such as abab) over a fixed, finite ",
              "alphabet (a predefined set of symbols such as {a, b}), and a language is some countable set of strings over an alphabet (such as ",
              "{abab, abbb, aa}). Therefore, when I talk about the \"expressive power\" of a machine, I'm really just referring to the languages ",
              "that such a machine is able to accept. As such, a large part of this course was really just defining and devising machines with increasing ",
              "levels of expressive power. We start with the basic: deterministic and nondeterministic finite automata (DFAs/NFAs). While these are a good ",
              "introduction to HOW exactly we define language acceptance for a machine (i.e. in terms of states & transitions). These, as the name ",
              "suggests, are both finite in memory (required to store symbols), and thus the same in expressive power (although NFAs are more convenient ",
              "to work with). To then give ourselves memory, we introduce Push-Down Automata (PDA). These are essentially the same as DFAs/NFAs, except ",
              "they now have exactly 1 infinite stack to store string symbols in the order they are received, and retrieve them in the reverse order. Are ",
              "there limitations for PDAs too though? Of course! But let's see how we can know this."
            ],
            "para2": [
              "We must prove that each type of machine really can't do what we claim it can't (for instance, no finite automaton can accept a language ",
              "where all strings contain an equal number, n, of a's and b's for n >= 0). Fortunately, as with most things, we stand on the shoulders of ",
              "giants. We use a very powerful tool, called the Pumping Lemma, that helps us prove either that a language is NOT regular (i.e. cannot be ",
              "accepted by a DFA/NFA) or NOT context-free (i.e. cannot be accepted by a PDA) depending on which variant we use. This is because all regular ",
              "/context-free languages satisfy the Pumping Lemma for regular/context-free languages. Thereby, the PL becomes a necessary (but not ",
              "sufficient) condition for regular/context-free languages. Finally, before commenting on Turing Machines, I note also two additional constructs ",
              "taught in this course: regular expressions, and grammars. In fact, to put it simply: regex, grammars, and automata are all varying ways ",
              "of expressing languages, and are closely related to each other (for example, (1) all regular expressions correspond exactly to the set of ",
              "of regular languages and can be expressed as a finite automaton or (2) context-free grammars - such as the one below - correspond exactly ",
              "to the set of context-free languages and can be expressed as a push-down automaton)."
            ],
            "para3": [
              "Turing Machines offer the highest level of expressive power. These gizmos consist minimally of tapes (with blank cells to write symbols into) ",
              "and a central finite control to do the actual reading, writing, and string acceptance/rejection (called halting for TMs). TMs offer the same ",
              "expressive power over all variants (single-tape; picture below, multi-tape, multi-track, etc), with these only bringing convenience-benefits ",
              "like NFAs do over DFAs. Once the course introduced the Turing Machine, it could then talk about much more sophisticated issues than just language ",
              "acceptance & rejection: namely, the decideability, and complexity of problems. I'll present just one undecideable problem, for the sake of brevity :), which is the ",
              "Halting Problem. This language involves the set of all arbitrary strings (w) and some Turing Machine (M1) and asks if M1 halts on w (i.e. accepts/",
              "rejects it in time finite. Is it obvious that this problem is undecidable (i.e. no TM halts on every string in the language)? If its not, then consider this informal ",
              "proof: suppose (*) there's some machine M2 that solves this problem, and run M2 in this way - [if M1 halts on input w, then keep M2 running ",
              "forever]. This exposes a contradiction since M2 won't halt whether or not M1 halts, but by supposition (*), M2 should've halted!. ",
              "There are many such undecideable problems of which some covered in this course are: The Diagonalisation Language, the Universal Language, ",
              "The Empty Language, etc. In fact, we were taught formal properties of a language as it relates to hardness, two being recursive enumerability ",
              "(i.e. a TM halts on all valid inputs for a language), and recursiveness (i.e. a TM halts on all inputs), which makes the undecideable languages effectively ",
              "at least non-recursive. Similarly, complexity - specifically, time/space costs - are defined similarly in a TM-computation sense, and ",
              "concomitantly so is the concept of NP/NP-Hard/NP-Complete. As a last remark, there is the actual Universal Turing Machine proposed originally ",
              "by Alan Turing - which claims to be able to decide any recursive language & compute any recursively enumerable language and that might be one of the ",
              "cooler parts of a load of content which is already groundbreaking to me."
            ]
          },
          {
            "code": "CS4226",
            "title": "Internet Architecture",
            "generalreview": "http://disq.us/p/3188z62",
            "meme": "https://i.imgflip.com/1rivut.jpg",
            "prologue": [""],
            "para1": [""],
            "para2": [""],
            "para3": [""]
          },
          {
            "code": "CS4234",
            "title": "Optimisation Algorithms",
            "generalreview": "http://disq.us/p/31898gp",
            "meme": "https://i.redd.it/vehc07hlgmr81.jpg",
            "prologue": [
              "The basic Algorithm Design course in NUS stops at defining NP-Hardness (recall: a problem which can be reduced in polynomial time from ",
              "any problem that is solvable in non-deterministic polynomial time i.e. the NP class), and introducing the P=NP problem (can any problem ",
              "in NP be solved in deterministic polynomial time?). Optimisation Algorithms then takes the baton from here, and proposes three relaxations ",
              "for optimising (minimising/maximising) over a specific subset of NP-Hard problems called combinatorial problems. These relaxations are over ",
              "(1) runtime (accept a reasonably nonpolynomial solution), (2) correctness (accept a reasonable approximation of the optimal solution), ",
              "and (3) generality (solve only special cases of an NP-Hard problem in deterministic polynomial time)."
            ],
            "para1": [
              "The concept of approximating the solution to a combinatorial optimisation problem was introduced using Minimum Vertex Cover (recall: ",
              "given vertices V and edges E, find a subset of V of minimum cardinality such that it includes at least 1 endpoint of every edge E). MVC ",
              "is known to be NP-Hard but, as stated above, an optimal solution can be approximated through several methods (a greedy 2-approximation ",
              "is a notable one). A 2-approximation of VC would therefore mean that the computed vertex cover has at most twice as many vertices ",
              "as the optimal one. Equally, if we wanted to maximise over something, then a 2-approximation would intuitively mean at least half of the ",
              "the optimal solution is achieved. Note that an approximation can be within a constant (such as 2 or 3) or function (such as logn). Some ",
              "problems are in-fact hard even to polynomially approximate, such as the Euclidean+No-Repeats Travelling Salesman Problem. Luckily these, ",
              "are few and far between in terms of practical use in the real world. The course also covered approximations for a host of problems including ",
              "but not limited to - Minimum Set Cover, the Travelling Salesman Problem, CNF-SAT, and Maximum Cut. In the end, my favorite approximation ",
              "algorithm was the Christofides-Serdyukov 1.5-approximation algorithm for TSP. I surmise I will forever be in awe of how this 12-step algorithm ",
              "that itself made use of minimum spanning trees, euclidean cycles, and edmonds' matching was even conceived of! Perhaps \"favorite\" is a ",
              "bit of a misnomer though, since I absolutely also loved Linear Programming which is a very simple technique that involves representing ",
              "the constraints and features of an combinatorial optimisation problem as a linear system, and solving it. Of course, since these problems ",
              "remain NP-Hard even for the linear system, the Linear Program often also has to be relaxed to retain the polynomial time bound."
            ],
            "para2": [
              "Separately, solving only a special case of an NP-Hard problem in polynomial time was illustrated using Maximum Matching (which, by the way, is ",
              "a mirror problem to Minimum Vertex Cover) but only in Bipartite Graphs. To work our way up to this though, we started with a very famous ",
              "theorem in optimisation: Maximum Flow-Minimum Cut (MFMC). This theorem states that in a network comprised of links each with some nonzero capacity, ",
              "a source s and a sink t, the maximum flow (i.e. the max outflow from s/inflow to t) is equal to the minimum cut (i.e. the set of edges with ",
              "minimum sum capacity that need to be removed to disconnect all paths from s to t). We first re-constructed the hallmark Ford-Fulkerson ",
              "method for finding the maximum flow/minimum cut of a network as well as practical implementations of this method (Edmonds-Karp Fat Pipes ",
              "& Dinitz's) which run in time polynomial (O(V^2E) to be explicit). Then, we applied MFMC (almost verbatim!) to special problems such as ",
              "maximum matching in bipartite graphs, minimum vertex-disjoint path cover, etc. It's great that we can do this since maximum bipartite matching ",
              "is itself a very powerful technique to easily solve combinatorial problems (for example: if I have X customers with Y sets of needs and Z ",
              "agents, how do I select the minimum number of agents such that all customers' needs are met?)."
            ],
            "para3": [
              "Notice that runtime, correctness, and generality are usually traded-off for each other i.e. BECAUSE we are okay with only an approximation ",
              "of an optimal solution, we can sketch out a solution that runs faster (in this sense, \"solutions\" begin to resemble \"heuristics\"). Finally, ",
              "as an aside - the course also covered some miscalleneous content on probability-based methods. There was a quick recap of naive randomization",
              "(often involved crudely assigning probabilities to events based on an intended outcome). But of course, we went beyond to study more refined ",
              "tricks such as the Lovász Local Lemma (which places a bound on the probability space by judging the indepence of certain events, and ",
              "derandomization using the method of conditional probabilities (very cool! almost a hack). I am, however, looking forward to NUS' course on ",
              "Randomized Algorithms (CS5330) to find out more beyond what little Optimisation has covered. All in all, a very encyclopedic course in ",
              "terms of algorithms for the prominent NP-Hard problems, and while needing to prove every claim was several margins tougher than the expectations ",
              "of the basic Algorithm Design course, it was still extremely rewarding trying to piece together simple-but-effective solutions to a host ",
              "of very engaging exercises. "
            ]
          }
        ],
        "NUSC": [
          {
            "code": "NHS2076",
            "title": "What can I know? (W.I.P.)",
            "generalreview": "",
            "meme": "https://images7.memedroid.com/images/UPLOADED918/6741d04540036.jpeg",
            "prologue": [""],
            "para1": [""],
            "para2": [""],
            "para3": [""]
          },
          {
            "code": "NEX3003",
            "title": "GEx New York City (W.I.P.)",
            "generalreview": "",
            "meme": "https://i.chzbgr.com/full/9835406336/hF3B6E70C/brotpoints-toughenke-30-12-17-baculofa-emery-aspt-cool-would-be-astoria-park-randalls-island-park",
            "prologue": [
              "The NUS College Global Experience (GEx) programmes are opportunities for students in NUS Residential Colleges to travel regionally and, ",
              "internationally to immerse themselves both personally & academically in foreign cultures. I had chosen New York City for GEx, for which ",
              "I stayed in an accommodation under Barnard College (616 116th Street) from 30th June - 27th July 2024. The NYC programme was a study on ",
              "Global Cities; specifically, the relationship between the \"Lived\" (Cité) and \"Built\" (Ville) as it relates uniquely to NYC. "
            ],
            "para1": [
              "Frankly, I've always wanted to visit NYC or California given how much exposure I received to these places through popular media in almost ",
              "all stages of life. In-fact, life itself seemed much larger in these places, and this would of-course agree with the topic of the GEx as ",
              "well. "
            ],
            "para2": [""],
            "para3": [""]
          }
        ]
      }
    }
  ],
  "Problems": {
    "TwoSum": {
      "title": "Two Sum",
      "topics": ["array", "hashmap"],
      "languages": ["PYTHON", "JAVA"],
      "description": [
        "Given an array of integers and a target, determine if there are 2 integers at distinct indices that add up to the target and return their indices. ",
        "You may assume there is exactly 1 unique solution per input."
      ],
      "example": [
        "Input: List - [2, 7, 4, 3, 1] with Target 9. Expected Output: [0, 1]"
      ],
      "solution": [
        "The naive solution to this problem is to check every pair of numbers to see if any sum up to our target. Obviously, this is rather inefficient since for ",
        "10 numbers we would be doing a 100 operations and 10000 for a 100 (i.e. we would be performing n^2 operations for an array of size n). Hence, let's use a ",
        "simple algorithm that can only needs to check n numbers to output a result (i.e. 1-pass through the array). The idea is: we just need to record the difference ",
        "between our target and each member of the array, and check if that difference (i.e. the initial number's 'complement') also exists in the array as a member. Using ",
        "the above example, we would take 2 and create a mapping (7 : 0) in our table. Why 0? Simply because the question wants us to track the indices of the desirable pair, ",
        "so we record 0 for 2. Then, when we get to the next number i.e. 7, we check if 7 could've been combined with a ",
        "previous number to make 9. And we know it can! Since we just recorded 7 for 2. Now, we simply extract the index of the other number (0 for 2 in this case) and return ",
        "the current index (i.e. 1, for 7) with the retrieved index (0) and return [0, 1]. Fin."
      ],
      "analysis": [
        "The reason this solution works is because of the use of 'maps'. We record the exact balance each number needs to sum up to the target, and then terminate as soon as we find ",
        "a match (which can be checked for quickly due to the efficient indexing that maps provide). If we manage to get through the whole array without a single match, we know ",
        "for sure that no number can meet its target using only other numbers in the array. Put another way: we know that no number in the array can be added to another ",
        "to reach the target value. Checking through all possible pairs of values is simply unnecessary for us to conclude that a particular number will never be useful in meeting ",
        "the given target."
      ],
      "image": "https://strapi-iio.s3.us-west-2.amazonaws.com/two_sum_1_3cf64db019.png"
    },
    "MergeTwoSortedLists": {
      "title": "Merge Two Sorted Lists",
      "topics": ["linked list"],
      "languages": ["JAVA"],
      "description": [
        "Given two linked lists sorted in ascending order, merge all the elements of the two lists into one linked list while preserving the order."
      ],
      "example": [
        "Input: LList 1, (1 -> 2 - > 4) and LList 2, (1 -> 3 -> 4). Expected Output: LList 3, (1 -> 1 -> 2 -> 3 -> 4 -> 4)."
      ],
      "solution": [
        "This is actually simpler than TwoSum. It's just a matter of traversing through both lists, comparing the heads of the lists at each value and slotting the smaller ",
        "value into the new list until both of the lists are exhausted. The only nuance to handle is: what if the two lists are not of equal length? In this case, we ",
        "just have to stop looking at the shorter list once its done, and just append the remaining whole of the longer list to the result list (which we can do since we are ",
        "assured that both lists are fully sorted. Additionally, why do we need ",
        "to maintain 2 lists 'curr' and 'result'? This is because to build a linked list iteratively, we need to keep shifting our list pointer until it reaches the tail. But ",
        "the solution demands the head of the resulting list. So, either we build only one list and spend extra time at the end shifting our pointer in reverse until it gets back ",
        "to the head. Or we can just use 2 lists, and fix one list at the head while using the other list to store the desired final values before appending the second ",
        "list to the first, and then returning the first. Fin."
      ],
      "analysis": [],
      "image": "https://favtutor.com/articles/wp-content/uploads/2023/12/Merge-two-sorted-linked-lists.png"
    },
    "BestTimeToBuyAndSellStock": {
      "title": "Best Time To Buy And Sell Stock",
      "topics": ["array", "dynamic programming"],
      "languages": ["PYTHON"],
      "description": [
        "Given a list of prices of a given stock from Day 1 to Day N, determine the maximum profit that can be ",
        "earned from buying and then selling the stock (with a profit of 0 if the stock price is only decreasing)."
      ],
      "example": [
        "Input: List - [7, 1, 5, 3, 6, 4]. Expected max profit: 5 (buy on day 2, sell on day 5)."
      ],
      "solution": [
        "Simply put, to maximise our profit we need to buy at the minimum price and sell at the maximum price. ",
        "A simplistic way to do this would be to compare all pairs of prices with another and take the pair with the highest ",
        "difference as long as the day we are buying is before the day we are selling. However, we can be a little smarter by realising ",
        "that each daily stock price can either be a new minimum or a new maximum i.e. we could just assume the first price we see is the ",
        "lowest we can get and then hope that either (1) we encounter a higher price later on so we may sell for some non-zero profit or ",
        "(2) encounter a lower price than the current minimum so that we can buy lower (i.e. at a better price). Notice however, that the usefulness of (2) is ",
        "contingent on (1) i.e. that the lower buy-price we get can still be later sold at a price that nets us some profit. If the stock price ",
        "is only decreasing, then case (1) will never happen (because there will never be a higher price than the first), and consequently case (2) ",
        "will not be of any use (because the price hits a new low everyday, with no opportunity to sell for a profit). "
      ],
      "analysis": [
        "This is a very simple example of what is called a 'greedy' algorithm because of its linearity, and the only aim being to maximise profit at the lowest price ",
        "we could get. The linearity of the algorithm is also assured by the linearity of time, in that we must buy before we sell (hence, we ",
        "just need to consider each day's price and its potential to maximise profit by being either a new low, or a new high)."
      ],
      "image": "https://www.ggorantala.dev/content/images/size/w1460/2024/02/Best-time-to-buy-and-sell-stock-1.png"
    },
    "LinkedListCycle": {
      "title": "Linked List Cycle",
      "topics": ["linked list", "two pointers"],
      "languages": ["C"],
      "description": [
        "Determine if a given linked list contains a cycle (i.e. there is some node in the list that can be reached twice ",
        "by continuously following the links between each pair of nodes consecutively)."
      ],
      "example": [
        "Input: LList (3 -> 2 -> 0 -> 4 -> 2). Expected answer: True, because the last node (4) leads back to the 2nd ",
        "node (2), and the cycle continues."
      ],
      "solution": [
        "We will use what is called the Hare-and-Tortoise or Fast-and-Slow pointer approach. The basic idea is simple: ",
        "imagine setting off 2 runners at the same start point, and checking if one runner is able to lap the other ",
        "before the race is over. Intuitively, if the faster runner reaches the endpoint without passing the slower runner ",
        "again (or in fact, reaches the end at all), this means there is never a repetition of nodes i.e. a point where the ",
        "faster runner crosses the slower one again. ",
        "The translation to code from this analogy is almost exact: initialise both pointers at the head of the list, and ",
        "always keep 1 pointer 2 steps ahead of the other until either both meet again (in which case there is a cycle) or ",
        "the advanced pointer reaches the end of the list (in which case, by definition there is no cycle)."
      ],
      "analysis": [
        "An interesting question to ask is: how to determine exactly how much faster the faster runner should be? In the above ",
        "code for example, why is the faster pointer kept 2 nodes ahead instead of 1, or 3? Well, really the requirement is to ",
        "prevent 2 situations: 'chasing-the-tail' (in which both pointers are perpetually one node apart from one another), ",
        "and 'teleporting' (in which the faster pointer is frequently overshooting the slower pointer). It should be obvious ",
        "that the first situation is a result of the faster pointer not being fast enough (never closing the distance to the ",
        "slower pointer), and the second is the opposite. Thus, if we maintain the slower node at a pace of 1 node per step, ",
        "and the faster node at a pace of 2 nodes per step, then the faster node would logically be steadily ",
        "closing the distance between them by 1 node at a time, and it would take exactly d steps to close an ",
        "initial distance of d nodes. Now, we can probably still trial/error a ratio other than 1:2 that would still work, but let's ",
        "keep it simple, stupid."
      ],
      "image": "https://static.takeuforward.org/content/starting-of-loop-image8-jG7vA1Si"
    },
    "BalancedBinaryTree": {
      "title": "Balanced Binary Tree",
      "topics": ["binary tree", "depth-first search"],
      "languages": ["PYTHON"],
      "description": [
        "A binary tree is height-balanced if the heights of the two subnodes of each node never differ by more than 1. ",
        "The height of a node is equal to the length of the longest path of sub-nodes from the node (inclusive), ",
        "to a leaf. Given any binary tree, assess if the tree is height-balanced or not."
      ],
      "example": [
        "Input: BinTree (in level-order): [1, 2, 3, 4, 5, null, null, 6, 7]. Expected answer: False, because the left ",
        "subnode (2) of the root (1) has height 3 (2 -> 4 -> 6 -> null) whereas the right subnode (3) has height 1 (3 -> null) "
      ],
      "solution": [
        "Let us adapt Depth-First Search (DFS; a form of search that explores each node to its complete depth/terminus before ",
        "moving on to a different node) to solve this problem. As per regular DFS, we use a stack and a visited set to explore ",
        "the tree's nodes. The modification would then be to also calculate the heights of each node as we explore. But how do we ",
        "go about doing this? Quite simply, we start counting the height of each node in a \"bottom-up\" fashion i.e. once ",
        "we reach a leaf, we mark its height as 1 and then as we revisit each of its predecesors, we record the consolidated height ",
        "for each of them based on the max (because height is defined as the \"longest\" path from node to bottom) heights of the successors'",
        "whose heights we've just calculated (this is where the bottom-up nature of DFS helps). If, at any point of our bottom-up traversal, we've ",
        "observed that any node is unbalanced in height - i.e. the consolidated heights of its successors/subnodes differ by ",
        "more than 1 - we can immediately conclude that the tree is unbalanced. In any other case, we simply finish calculating the ",
        "heights of all the nodes in the tree (with the final one being the height of the root node), and then conclude that the ",
        "tree is balanced."
      ],
      "analysis": [
        "This is an iterative (i.e. bottom-up) solution. The recursive (i.e. top-down solution) is easier in terms of code-complexity, but may be more expensive ",
        "in terms of the space required to maintain the call stack (i.e. each recursive invocation & its parameters etc.). In fact, the ",
        "iterative solution - while lengthier code-wise - is (as mentioned) mostly boilerplate  DFS code, with the simple modification ",
        "of explicitly consolidating the height of each node instead of relying on the program's call stack to do it. The \"base-case\" for this solution is the ",
        "same as for the recursive solution, which is either if the given tree is empty (in which case it is trivially balanced), or if we've ",
        "bottomed-out on calculating the depth of any node (i.e. the node has no further subnodes that can contribute to its height)."
      ],
      "image": "https://media.geeksforgeeks.org/wp-content/uploads/20240805164549/balance-vs-unbalance-binnary-tree.webp"
    },
    "ReverseLinkedList": {
      "title": "Reverse Linked List",
      "topics": ["linked list"],
      "languages": ["PYTHON"],
      "description": [
        "Given a linked list, rearranged the nodes of the list in reversed order."
      ],
      "example": [
        "Input: LList (1 -> 2 -> 3 -> 4 -> 5 -> º) becomes (5 -> 4 -> 3 -> 2 -> 1 -> º) where º marks the end of the list, i.e. ",
        "the presence of no further nodes."
      ],
      "solution": [
        "This is a simple problem that makes somewhat creative uses of iterative & recursive approaches. The iterative, ",
        "approach in this case is, in my view, easier to grasp: start with the the first node, \"push it all the way to the back\", and then  ",
        "sequentially swap the latter nodes to the front. Using the above example, we start with (1) and º, and create (1 -> º), after ",
        "which we connect 2 to (1 -> º) to create (2 -> 1 -> º), and so on until we have 5 in hand, and the list we've constructed up till ",
        "then is (4 -> 3 -> 2 -> 1 -> º) which leaves the only remaining step to set (5 -> 4 -> 3 -> 2 -> 1 -> º). The process here comes ",
        "down to 4 steps: (1) note down what the neighbor of the current node is, (2) switch the current node's neighbor (->) to point to the newly ",
        "reversed list up to that node, (3) set the head of the list to the (newly connected) current node, and finally (4) move onto the next ",
        "node to be swapped to the front. Obviously, this would continue until we've reached the end of the original list, since at that ",
        "point we should've reversed all the nodes from first to last. The recursive solution, while not a departure from the ",
        "basics of any recursive solution (having base & inductive cases), is a little trickier to spot. The idea is still the same however: ",
        "successively swap the nodes from end to start to reverse the list. Reusing the same ",
        "example list, we traverse to the end (5), then step back to (4) to create (5 -> 4), then to (3) to create (5 -> 4 -> 3), and so on. The ",
        "only real difference between the two solutions remains that the iterative solution is bottom-up (i.e. we start with 1 -> º, then ",
        "2 -> 1 -> º...) whereas the recursive one is top-down (i.e. we start with 5, then 5 -> 4, then 5 -> 4 -> 3...)"
      ],
      "analysis": [
        "I expect this problem emphasises that the fundamentals of recursive & iterative solutions don't change all too much. ",
        "When writing a recursive solution, we start with the base case (in this case, if the list is empty or only contains one node), ",
        "and then construct the inductive case using wishful thinking (in this case - assuming we already have a fully reversed list for ",
        "every other node except the first two, and then just swapping those particular two manually - a step which if applied for every pair of nodes ",
        "recursively, will eventually result in the actual fully reversed list. When writing an iterative solution, we start with the same base case, ",
        "and then traverse through the list in-order while swapping one node's position at a time such that by the start and end of 1 loop, ",
        "we know that exactly one more node has been brought to the front of our list thus far."
      ],
      "image": "https://www.boardinfinity.com/blog/content/images/2022/10/reverse-a-linked-list-image-cover.png"
    },
    "MajorityElement": {
      "title": "Majority Element",
      "topics": ["array", "math"],
      "languages": ["PYTHON"],
      "description": [
        "Given a list of numbers, identify the majority element i.e. the number that appears at least half as many times as there are ",
        "numbers in the list. It is guaranteed that there is at least 1 majority element in the list."
      ],
      "example": [
        "Input: List - for [1, 1, 1, 2, 2], the majority element is 1."
      ],
      "solution": [
        "The obvious solution would be to simply go through the list and maintain the frequency of each type of number. However, is there a ",
        "way to solve this problem without needing the extra space to maintain the count of each number? Fortunately, yes! The obvious observation ",
        "to make is that for a number to be a majority element, it has to occupy more space than is occupied by numbers that are not that ",
        "number. So really, all that has to be done is to maintain a counter, and check which number \"survives\" the search i.e. occupies ",
        "at least 1 more space in total than any other number in the list. So, using [1, 1, 1, 2, 2] as an example - the counter starts with ",
        "0 and the majority element is assumed to be 1 (as a start). Then, as we get to three 1s, the counter gets to 3 (i.e. we know 1 occupies ",
        "3 spaces in the list) whereas when we see two 2s, the counter is reduced by 1 each time until it sinks to 1 (i.e. we know that 1 only occupies 1 more space than ",
        "any other number(s) in the list. However, since we terminate our search with the counter at 1, and majority element still 1, it is ",
        "good enough for us to conclude that 1 is the majority element. If, instead we had [1, 1, 2, 2, 2] - then we might still start ",
        "with counter = 0 and majority element = 1, but eventually the two 1s (counter = 2) would be tipped by the three 2s and we would ",
        "end with counter = 1 and majority element = 2, and thus learn that the 2s occupy 1 more space than any other number(s) in the list."
      ],
      "analysis": [],
      "image": "https://files.prepinsta.com/2023/03/Majority-Element-In-An-Array-1024x832.webp"
    },
    "CountingBits": {
      "title": "Counting Bits",
      "topics": ["bit manipulation", "dynamic programming"],
      "languages": ["PYTHON"],
      "description": [
        "Given an integer n, compute - in a list - how many 1s are present in the binary representation of every number from 0 to n."
      ],
      "example": [
        "Input: For n = 5, the expected output list is [0, 1, 1, 2, 1, 2] for 0 <-> 0, 1 <-> 1, 2 <-> 10, 3 <-> 11, 4 <-> 100, and 5 <-> 101."
      ],
      "solution": [
        "Do we have to compute the binary representation of every single number from 0 to n? That seems like a necessity at first in ",
        "order to know how many 1s are in each number. But in fact it isn't necessary, and not needing to do that actually makes our solution ",
        "so much easier - we use the power of bit manipulation, and dynamic programming. First, is it obvious that the number of 1s in a ",
        "number X, is nearly the same as the number of 1s in X/2 except for a possible 1-bit difference? In general, the number of bits ",
        "in a number and twice of the same number, differs by only 1 bit - and that bit is either 1, or 0 (obviously); the implication of this is that there ",
        "could only be a difference of 1 in the number of 1s between X and X/2. Further, we can even determine whether the extra bit is a 0 or 1, by checking ",
        "the parity of the number. If its odd, then the extra bit would be 1, otherwise it would be 0; notice the difference between 2 (10), ",
        "4 (100), and 5 (101). This is how bit manipulation is a big help, since using the relation between the binary representations ",
        "of a number and half of itself - we can form the rule that the number of 1s in a number is equal to the number of 1s in half of itself ",
        "+ an additional 1 if the given number is odd (and thus the extra bit is 1). However, this rule is obviously recursive, since to know the ",
        "number of 1s in a number, one would need the number of 1s in half of that number, and so on & so forth. This is what now makes this a ",
        "dynamic programming problem - wherein we start with a base case (0 <-> 0) and use the info we got from each number to construct ",
        "the results for subsequent numbers."
      ],
      "analysis": [],
      "image": "https://blog.kakaocdn.net/dn/2omqt/btrFIeDHAp6/YxZ7ajJC1xSMZE220A8Fuk/img.jpg"
    },
    "LongestCommonPrefix": {
      "title": "Longest Common Prefix",
      "topics": ["string", "sorting"],
      "languages": ["PYTHON"],
      "description": [
        "Given a list of strings, determine their longest common prefix."
      ],
      "example": [
        "Input: For list [\"flight\", \"flower\", \"flow\"], the longest common prefix is \"fl\"."
      ],
      "solution": [
        "To determine the longest common prefix, we need to check 2 things: (1) that all strings include a common group of characters (which would later form ",
        "the prefix), and (2) that these characters appear in the same contiguous positions in all strings. The 2nd requirement is easy to fulfil - ",
        "just comparing strings sequentially (i.e. in-parallel at the same positions/indices) would do. ",
        "But how to fulfil the 1st requirement in a relatively efficient & simple manner? This is where sorting helps us! To start, we sort ",
        "all of the strings in lexicographical order (the order in the above example would be [\"flight\", \"flow\", \"flower\"]. Next, we ",
        "compare the smallest and largest strings, and return their common prefix (if any), and then we are actually done! By sorting, we've reduced our ",
        "domain of comparison from all strings, to just 2. But why does this even work? Well, let's think of 2 cases. The first is ",
        "that there are no characters in common between the first and last string - in which case, the longest common prefix between all strings ",
        "is trivially empty. The second is that there is a character x that is common between the first and last string at position i. Is it possible that the ",
        "first and last string have x in common at i but no other string does? Well, no because this is where sorting comes into play. ",
        "If the first and last string have a character x in common at position i that no string in-between does, this means the first string is actually not the ",
        "smallest string. Why is this the case? Suppose the character in position i for a string in-between is y. Since x is present at i for ",
        "the largest string, this means x > y. This means that any string with x in it at position i, must come after all strings with y (or smaller) ",
        "characters at position i - which obviously makes the smallest string, not in-fact the smallest. This is of course a contradiction since ",
        "we know we already sorted the list lexicographically. Think of an example: [\"ad\", \"ab\", \"ac\", \"ad\"]. The first and last strings seem to share ",
        "a character \"d\" in the 2nd position that no string in-between does, but this list is not even lexicographically sorted. If it were, ",
        "then the first string would be \"ab\", and the longest common prefix would be correctly identified as \"a\". Obviously, once the first ",
        "and last string are mismatched (in this case, at \"b\" and \"d\", we can return the prefix we have so far (since again, a prefix has to be contiguous ",
        "and once the chain is broken anywhere the prefix ends there too."
      ],
      "analysis": [
        "This solution may not necessarily be more efficient than more brute-force solutions (such as comparing the characters of all ",
        "strings in every position). However, it is definitely much simpler in-code, and not necessarily much worse in performance. Debugging ",
        "and testing also tends to be easier with simpler solutions - which is a genuine consideration!"
      ],
      "image": "https://www.interviewbit.com/blog/wp-content/uploads/2021/10/Horizantal-Scanning-Approach.png"
    }
  }
}
